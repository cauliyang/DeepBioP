{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"DeepBioP","text":"<p>Deep Learning Processing Library for Biological Data</p>"},{"location":"#setup","title":"Setup","text":""},{"location":"#python","title":"Python","text":"<p>install the latest deepbiop version with:</p> <pre><code>pip install deepbiop\n</code></pre>"},{"location":"#rust","title":"Rust","text":"<p>You can take the latest release from <code>crates.io</code>, or if you want to use the latest features / performance improvements point to the <code>main</code> branch of this repo.</p> <pre><code>cargo add deepbiop --features fq\n</code></pre> <p>Each enabled feature can then be imported by its re-exported name, e.g.,</p> <pre><code>use deepbiop::fastq;\n</code></pre>"},{"location":"#cli","title":"CLI","text":"<pre><code>cargo install deepbiop-cli\ndbp -h\n</code></pre>"},{"location":"#minimum-supported-rust-version-msrv","title":"Minimum Supported Rust Version (MSRV)","text":"<p>This project adheres to a Minimum Supported Rust Version (MSRV) policy. The Minimum Supported Rust Version (MSRV) is 1.70.0. We ensure that all code within the project is compatible with this version or newer to maintain stability and compatibility.</p>"},{"location":"#contribute","title":"Contribute \ud83e\udd1d","text":"<p>Call for Participation: Deep Learning Processing Library for Biological Data</p> <p>We are excited to announce the launch of a new open-source project focused on developing a cutting-edge deep learning processing library specifically designed for biological data. This project aims to empower researchers, data scientists, and developers to leverage the latest advancements in deep learning to analyze and interpret complex biological datasets.</p> <p>Project Overview:</p> <p>Biological data, such as genomic sequences, proteomics, and imaging data, presents unique challenges and opportunities for machine learning applications. Our library seeks to provide a comprehensive suite of tools and algorithms that streamline the preprocessing, modeling, and analysis of biological data using deep learning techniques.</p> <p>Key Features:</p> <ul> <li>Data Preprocessing: Efficient tools for cleaning, normalizing, and augmenting biological data.</li> <li>Model Building: Pre-built models and customizable architectures tailored for various types of biological data.</li> <li>Visualization: Advanced visualization tools to help interpret model predictions and insights.</li> <li>Integration: Seamless integration with popular bioinformatics tools and frameworks.</li> <li>APIs: Rust and Python APIs to facilitate easy integration with different deep learning frameworks, ensuring efficient operations across platforms.</li> </ul> <p>Who Should Participate?</p> <p>We invite participation from individuals and teams who are passionate about bioinformatics, deep learning, and open-source software development. Whether you are a researcher, developer, or student, your contributions can help shape the future of biological data analysis.</p> <p>How to Get Involved:</p> <ul> <li>Developers: Contribute code, fix bugs, and develop new features.</li> <li>Researchers: Share your domain expertise and help validate models.</li> <li>Students: Gain experience by working on real-world data science problems.</li> <li>Community Members: Provide feedback, report issues, and help grow the user community.</li> </ul> <p>Join Us:</p> <p>If you are interested in participating, please visit our GitHub repository at GitHub to explore the project and get started.</p> <p>Contact Us:</p> <p>For more information or questions, feel free to contact us at [yangyang.li@norwestern.edu]. We look forward to your participation and contributions to this exciting project!</p> <p>Together, let's advance the field of biological data analysis with the power of deep learning!</p>"},{"location":"license/","title":"License","text":"<pre><code>                             Apache License\n                       Version 2.0, January 2004\n                    http://www.apache.org/licenses/\n</code></pre> <p>TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION</p> <ol> <li> <p>Definitions.</p> <p>\"License\" shall mean the terms and conditions for use, reproduction,   and distribution as defined by Sections 1 through 9 of this document.</p> <p>\"Licensor\" shall mean the copyright owner or entity authorized by   the copyright owner that is granting the License.</p> <p>\"Legal Entity\" shall mean the union of the acting entity and all   other entities that control, are controlled by, or are under common   control with that entity. For the purposes of this definition,   \"control\" means (i) the power, direct or indirect, to cause the   direction or management of such entity, whether by contract or   otherwise, or (ii) ownership of fifty percent (50%) or more of the   outstanding shares, or (iii) beneficial ownership of such entity.</p> <p>\"You\" (or \"Your\") shall mean an individual or Legal Entity   exercising permissions granted by this License.</p> <p>\"Source\" form shall mean the preferred form for making modifications,   including but not limited to software source code, documentation   source, and configuration files.</p> <p>\"Object\" form shall mean any form resulting from mechanical   transformation or translation of a Source form, including but   not limited to compiled object code, generated documentation,   and conversions to other media types.</p> <p>\"Work\" shall mean the work of authorship, whether in Source or   Object form, made available under the License, as indicated by a   copyright notice that is included in or attached to the work   (an example is provided in the Appendix below).</p> <p>\"Derivative Works\" shall mean any work, whether in Source or Object   form, that is based on (or derived from) the Work and for which the   editorial revisions, annotations, elaborations, or other modifications   represent, as a whole, an original work of authorship. For the purposes   of this License, Derivative Works shall not include works that remain   separable from, or merely link (or bind by name) to the interfaces of,   the Work and Derivative Works thereof.</p> <p>\"Contribution\" shall mean any work of authorship, including   the original version of the Work and any modifications or additions   to that Work or Derivative Works thereof, that is intentionally   submitted to Licensor for inclusion in the Work by the copyright owner   or by an individual or Legal Entity authorized to submit on behalf of   the copyright owner. For the purposes of this definition, \"submitted\"   means any form of electronic, verbal, or written communication sent   to the Licensor or its representatives, including but not limited to   communication on electronic mailing lists, source code control systems,   and issue tracking systems that are managed by, or on behalf of, the   Licensor for the purpose of discussing and improving the Work, but   excluding communication that is conspicuously marked or otherwise   designated in writing by the copyright owner as \"Not a Contribution.\"</p> <p>\"Contributor\" shall mean Licensor and any individual or Legal Entity   on behalf of whom a Contribution has been received by Licensor and   subsequently incorporated within the Work.</p> </li> <li> <p>Grant of Copyright License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       copyright license to reproduce, prepare Derivative Works of,       publicly display, publicly perform, sublicense, and distribute the       Work and such Derivative Works in Source or Object form.</p> </li> <li> <p>Grant of Patent License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       (except as stated in this section) patent license to make, have made,       use, offer to sell, sell, import, and otherwise transfer the Work,       where such license applies only to those patent claims licensable       by such Contributor that are necessarily infringed by their       Contribution(s) alone or by combination of their Contribution(s)       with the Work to which such Contribution(s) was submitted. If You       institute patent litigation against any entity (including a       cross-claim or counterclaim in a lawsuit) alleging that the Work       or a Contribution incorporated within the Work constitutes direct       or contributory patent infringement, then any patent licenses       granted to You under this License for that Work shall terminate       as of the date such litigation is filed.</p> </li> <li> <p>Redistribution. You may reproduce and distribute copies of the       Work or Derivative Works thereof in any medium, with or without       modifications, and in Source or Object form, provided that You       meet the following conditions:</p> <p>(a) You must give any other recipients of the Work or       Derivative Works a copy of this License; and</p> <p>(b) You must cause any modified files to carry prominent notices       stating that You changed the files; and</p> <p>(c) You must retain, in the Source form of any Derivative Works       that You distribute, all copyright, patent, trademark, and       attribution notices from the Source form of the Work,       excluding those notices that do not pertain to any part of       the Derivative Works; and</p> <p>(d) If the Work includes a \"NOTICE\" text file as part of its       distribution, then any Derivative Works that You distribute must       include a readable copy of the attribution notices contained       within such NOTICE file, excluding those notices that do not       pertain to any part of the Derivative Works, in at least one       of the following places: within a NOTICE text file distributed       as part of the Derivative Works; within the Source form or       documentation, if provided along with the Derivative Works; or,       within a display generated by the Derivative Works, if and       wherever such third-party notices normally appear. The contents       of the NOTICE file are for informational purposes only and       do not modify the License. You may add Your own attribution       notices within Derivative Works that You distribute, alongside       or as an addendum to the NOTICE text from the Work, provided       that such additional attribution notices cannot be construed       as modifying the License.</p> <p>You may add Your own copyright statement to Your modifications and   may provide additional or different license terms and conditions   for use, reproduction, or distribution of Your modifications, or   for any such Derivative Works as a whole, provided Your use,   reproduction, and distribution of the Work otherwise complies with   the conditions stated in this License.</p> </li> <li> <p>Submission of Contributions. Unless You explicitly state otherwise,       any Contribution intentionally submitted for inclusion in the Work       by You to the Licensor shall be under the terms and conditions of       this License, without any additional terms or conditions.       Notwithstanding the above, nothing herein shall supersede or modify       the terms of any separate license agreement you may have executed       with Licensor regarding such Contributions.</p> </li> <li> <p>Trademarks. This License does not grant permission to use the trade       names, trademarks, service marks, or product names of the Licensor,       except as required for reasonable and customary use in describing the       origin of the Work and reproducing the content of the NOTICE file.</p> </li> <li> <p>Disclaimer of Warranty. Unless required by applicable law or       agreed to in writing, Licensor provides the Work (and each       Contributor provides its Contributions) on an \"AS IS\" BASIS,       WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or       implied, including, without limitation, any warranties or conditions       of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A       PARTICULAR PURPOSE. You are solely responsible for determining the       appropriateness of using or redistributing the Work and assume any       risks associated with Your exercise of permissions under this License.</p> </li> <li> <p>Limitation of Liability. In no event and under no legal theory,       whether in tort (including negligence), contract, or otherwise,       unless required by applicable law (such as deliberate and grossly       negligent acts) or agreed to in writing, shall any Contributor be       liable to You for damages, including any direct, indirect, special,       incidental, or consequential damages of any character arising as a       result of this License or out of the use or inability to use the       Work (including but not limited to damages for loss of goodwill,       work stoppage, computer failure or malfunction, or any and all       other commercial damages or losses), even if such Contributor       has been advised of the possibility of such damages.</p> </li> <li> <p>Accepting Warranty or Additional Liability. While redistributing       the Work or Derivative Works thereof, You may choose to offer,       and charge a fee for, acceptance of support, warranty, indemnity,       or other liability obligations and/or rights consistent with this       License. However, in accepting such obligations, You may act only       on Your own behalf and on Your sole responsibility, not on behalf       of any other Contributor, and only if You agree to indemnify,       defend, and hold each Contributor harmless for any liability       incurred by, or claims asserted against, such Contributor by reason       of your accepting any such warranty or additional liability.</p> </li> </ol> <p>END OF TERMS AND CONDITIONS</p> <p>APPENDIX: How to apply the Apache License to your work.</p> <pre><code>  To apply the Apache License to your work, attach the following\n  boilerplate notice, with the fields enclosed by brackets \"[]\"\n  replaced with your own identifying information. (Don't include\n  the brackets!)  The text should be enclosed in the appropriate\n  comment syntax for the file format. We also recommend that a\n  file or class name and description of purpose be included on the\n  same \"printed page\" as the copyright notice for easier\n  identification within third-party archives.\n</code></pre> <p>Copyright [yyyy] [name of copyright owner]</p> <p>Licensed under the Apache License, Version 2.0 (the \"License\");    you may not use this file except in compliance with the License.    You may obtain a copy of the License at</p> <pre><code>   http://www.apache.org/licenses/LICENSE-2.0\n</code></pre> <p>Unless required by applicable law or agreed to in writing, software    distributed under the License is distributed on an \"AS IS\" BASIS,    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.    See the License for the specific language governing permissions and    limitations under the License.</p>"},{"location":"api/","title":"API reference","text":"<p>The API reference contains detailed descriptions of all public functions and objects. It's the best place to look if you need information on a specific function.</p>"},{"location":"api/#python","title":"Python","text":"<p>The Python API reference is built using Sphinx. It's available in our docs.</p>"},{"location":"api/#rust","title":"Rust","text":"<p>The Rust API reference is built using Cargo. It's available on docs.rs.</p>"},{"location":"cli/","title":"Command-Line Help for <code>deepbiop-cli</code>","text":"<p>This document contains the help content for the <code>deepbiop-cli</code> command-line program.</p> <p>Command Overview:</p> <ul> <li><code>deepbiop-cli</code>\u21b4</li> <li><code>deepbiop-cli count-chimeric</code>\u21b4</li> <li><code>deepbiop-cli bam-to-fq</code>\u21b4</li> <li><code>deepbiop-cli fq-to-fa</code>\u21b4</li> <li><code>deepbiop-cli fa-to-fq</code>\u21b4</li> <li><code>deepbiop-cli fa-to-parquet</code>\u21b4</li> <li><code>deepbiop-cli fq-to-parquet</code>\u21b4</li> <li><code>deepbiop-cli extract-fq</code>\u21b4</li> <li><code>deepbiop-cli extract-fa</code>\u21b4</li> <li><code>deepbiop-cli fqs-to-one</code>\u21b4</li> <li><code>deepbiop-cli fas-to-one</code>\u21b4</li> </ul>"},{"location":"cli/#deepbiop-cli","title":"<code>deepbiop-cli</code>","text":"<p>CLI tool for Processing Biological Data.</p> <p>Usage: <code>deepbiop-cli [OPTIONS] [COMMAND]</code></p>"},{"location":"cli/#subcommands","title":"Subcommands:","text":"<ul> <li><code>count-chimeric</code> \u2014 Count chimeric reads in a BAM file</li> <li><code>bam-to-fq</code> \u2014 BAM to fastq conversion</li> <li><code>fq-to-fa</code> \u2014 Fastq to fasta conversion</li> <li><code>fa-to-fq</code> \u2014 Fasta to fastq conversion</li> <li><code>fa-to-parquet</code> \u2014 Fasta to parquet conversion</li> <li><code>fq-to-parquet</code> \u2014 Fastq to parquet conversion</li> <li><code>extract-fq</code> \u2014 Extract fastq reads from a fastq file</li> <li><code>extract-fa</code> \u2014 Extract fasta reads from a fasta file</li> <li><code>fqs-to-one</code> \u2014 Multiple Fastqs to one Fastq conversion</li> <li><code>fas-to-one</code> \u2014 Multiple Fastas to one Fasta conversion</li> </ul>"},{"location":"cli/#options","title":"Options:","text":"<ul> <li><code>--generate &lt;GENERATOR&gt;</code></li> </ul> <p>Possible values: <code>bash</code>, <code>elvish</code>, <code>fish</code>, <code>powershell</code>, <code>zsh</code></p> <ul> <li><code>-v</code>, <code>--verbose</code> \u2014 Increase logging verbosity</li> <li><code>-q</code>, <code>--quiet</code> \u2014 Decrease logging verbosity</li> </ul>"},{"location":"cli/#deepbiop-cli-count-chimeric","title":"<code>deepbiop-cli count-chimeric</code>","text":"<p>Count chimeric reads in a BAM file</p> <p>Usage: <code>deepbiop-cli count-chimeric [OPTIONS] [bam]...</code></p>"},{"location":"cli/#arguments","title":"Arguments:","text":"<ul> <li><code>&lt;bam&gt;</code> \u2014 path to the bam file</li> </ul>"},{"location":"cli/#options_1","title":"Options:","text":"<ul> <li><code>-t</code>, <code>--threads &lt;THREADS&gt;</code> \u2014 threads number</li> </ul> <p>Default value: <code>2</code></p>"},{"location":"cli/#deepbiop-cli-bam-to-fq","title":"<code>deepbiop-cli bam-to-fq</code>","text":"<p>BAM to fastq conversion</p> <p>Usage: <code>deepbiop-cli bam-to-fq [OPTIONS] [bam]...</code></p>"},{"location":"cli/#arguments_1","title":"Arguments:","text":"<ul> <li><code>&lt;bam&gt;</code> \u2014 path to the bam file</li> </ul>"},{"location":"cli/#options_2","title":"Options:","text":"<ul> <li><code>-t</code>, <code>--threads &lt;THREADS&gt;</code> \u2014 threads number</li> </ul> <p>Default value: <code>2</code> * <code>-c</code>, <code>--compressed</code> \u2014 output bgzip compressed fastq file</p>"},{"location":"cli/#deepbiop-cli-fq-to-fa","title":"<code>deepbiop-cli fq-to-fa</code>","text":"<p>Fastq to fasta conversion</p> <p>Usage: <code>deepbiop-cli fq-to-fa [OPTIONS] [fq]...</code></p>"},{"location":"cli/#arguments_2","title":"Arguments:","text":"<ul> <li><code>&lt;fq&gt;</code> \u2014 path to the fq file</li> </ul>"},{"location":"cli/#options_3","title":"Options:","text":"<ul> <li><code>-t</code>, <code>--threads &lt;THREADS&gt;</code> \u2014 threads number</li> </ul> <p>Default value: <code>2</code></p>"},{"location":"cli/#deepbiop-cli-fa-to-fq","title":"<code>deepbiop-cli fa-to-fq</code>","text":"<p>Fasta to fastq conversion</p> <p>Usage: <code>deepbiop-cli fa-to-fq [OPTIONS] [fa]...</code></p>"},{"location":"cli/#arguments_3","title":"Arguments:","text":"<ul> <li><code>&lt;fa&gt;</code> \u2014 path to the fa file</li> </ul>"},{"location":"cli/#options_4","title":"Options:","text":"<ul> <li><code>-t</code>, <code>--threads &lt;THREADS&gt;</code> \u2014 threads number</li> </ul> <p>Default value: <code>2</code></p>"},{"location":"cli/#deepbiop-cli-fa-to-parquet","title":"<code>deepbiop-cli fa-to-parquet</code>","text":"<p>Fasta to parquet conversion</p> <p>Usage: <code>deepbiop-cli fa-to-parquet [OPTIONS] &lt;fa&gt;</code></p>"},{"location":"cli/#arguments_4","title":"Arguments:","text":"<ul> <li><code>&lt;fa&gt;</code> \u2014 path to the fa file</li> </ul>"},{"location":"cli/#options_5","title":"Options:","text":"<ul> <li><code>--chunk</code> \u2014 if convert the fa file to parquet by chunk or not</li> <li><code>--chunk-size &lt;CHUNK_SIZE&gt;</code> \u2014 chunk size</li> </ul> <p>Default value: <code>1000000</code> * <code>--output &lt;result&gt;</code> \u2014 result path * <code>-t</code>, <code>--threads &lt;THREADS&gt;</code> \u2014 threads number</p> <p>Default value: <code>2</code></p>"},{"location":"cli/#deepbiop-cli-fq-to-parquet","title":"<code>deepbiop-cli fq-to-parquet</code>","text":"<p>Fastq to parquet conversion</p> <p>Usage: <code>deepbiop-cli fq-to-parquet [OPTIONS] &lt;fq&gt;</code></p>"},{"location":"cli/#arguments_5","title":"Arguments:","text":"<ul> <li><code>&lt;fq&gt;</code> \u2014 path to the fq file</li> </ul>"},{"location":"cli/#options_6","title":"Options:","text":"<ul> <li><code>--chunk</code> \u2014 if convert the fq file to parquet by chunk or not</li> <li><code>--chunk-size &lt;CHUNK_SIZE&gt;</code> \u2014 chunk size</li> </ul> <p>Default value: <code>1000000</code> * <code>--output &lt;output&gt;</code> \u2014 result path * <code>-t</code>, <code>--threads &lt;THREADS&gt;</code> \u2014 threads number</p> <p>Default value: <code>2</code></p>"},{"location":"cli/#deepbiop-cli-extract-fq","title":"<code>deepbiop-cli extract-fq</code>","text":"<p>Extract fastq reads from a fastq file</p> <p>Usage: <code>deepbiop-cli extract-fq [OPTIONS] &lt;fq&gt;</code></p>"},{"location":"cli/#arguments_6","title":"Arguments:","text":"<ul> <li><code>&lt;fq&gt;</code> \u2014 path to the fq file</li> </ul>"},{"location":"cli/#options_7","title":"Options:","text":"<ul> <li><code>--reads &lt;reads&gt;</code> \u2014 Path to the selected reads</li> <li><code>--number &lt;number&gt;</code> \u2014 The number of selected reads by random</li> <li><code>--output &lt;output&gt;</code> \u2014 output bgzip compressed file</li> <li><code>-t</code>, <code>--threads &lt;THREADS&gt;</code> \u2014 threads number</li> </ul> <p>Default value: <code>2</code> * <code>-c</code>, <code>--compressed</code> \u2014 output bgzip compressed fastq file</p>"},{"location":"cli/#deepbiop-cli-extract-fa","title":"<code>deepbiop-cli extract-fa</code>","text":"<p>Extract fasta reads from a fasta file</p> <p>Usage: <code>deepbiop-cli extract-fa [OPTIONS] &lt;fa&gt;</code></p>"},{"location":"cli/#arguments_7","title":"Arguments:","text":"<ul> <li><code>&lt;fa&gt;</code> \u2014 path to the bam file</li> </ul>"},{"location":"cli/#options_8","title":"Options:","text":"<ul> <li><code>--reads &lt;reads&gt;</code> \u2014 Path to the selected reads</li> <li><code>--number &lt;number&gt;</code> \u2014 The number of selected reads by random</li> <li><code>--output &lt;output&gt;</code> \u2014 output bgzip compressed file</li> <li><code>-t</code>, <code>--threads &lt;THREADS&gt;</code> \u2014 threads number</li> </ul> <p>Default value: <code>2</code> * <code>-c</code>, <code>--compressed</code> \u2014 output bgzip compressed fasta file</p>"},{"location":"cli/#deepbiop-cli-fqs-to-one","title":"<code>deepbiop-cli fqs-to-one</code>","text":"<p>Multiple Fastqs to one Fastq conversion</p> <p>Usage: <code>deepbiop-cli fqs-to-one [OPTIONS] --output &lt;output&gt; [fqs]...</code></p>"},{"location":"cli/#arguments_8","title":"Arguments:","text":"<ul> <li><code>&lt;fqs&gt;</code> \u2014 path to the fq file</li> </ul>"},{"location":"cli/#options_9","title":"Options:","text":"<ul> <li><code>--output &lt;output&gt;</code> \u2014 output bgzip compressed file</li> <li><code>-t</code>, <code>--threads &lt;THREADS&gt;</code></li> </ul> <p>Default value: <code>2</code></p>"},{"location":"cli/#deepbiop-cli-fas-to-one","title":"<code>deepbiop-cli fas-to-one</code>","text":"<p>Multiple Fastas to one Fasta conversion</p> <p>Usage: <code>deepbiop-cli fas-to-one [OPTIONS] --output &lt;output&gt; [fas]...</code></p>"},{"location":"cli/#arguments_9","title":"Arguments:","text":"<ul> <li><code>&lt;fas&gt;</code> \u2014 path to the fa file</li> </ul>"},{"location":"cli/#options_10","title":"Options:","text":"<ul> <li><code>--output &lt;output&gt;</code> \u2014 output bgzip compressed file</li> <li><code>-t</code>, <code>--threads &lt;THREADS&gt;</code></li> </ul> <p>Default value: <code>2</code></p> <p>     This document was generated automatically by     <code>clap-markdown</code>. </p>"},{"location":"cli/cli/","title":"Command-Line Help for <code>deepbiop-cli</code>","text":"<p>This document contains the help content for the <code>deepbiop-cli</code> command-line program.</p> <p>Command Overview:</p> <ul> <li>Command-Line Help for <code>deepbiop-cli</code></li> <li><code>deepbiop-cli</code>           - Subcommands:           - Options:</li> <li><code>deepbiop-cli count-chimeric</code>           - Arguments:           - Options:</li> <li><code>deepbiop-cli bam-to-fq</code>           - Arguments:           - Options:</li> <li><code>deepbiop-cli fq-to-fa</code>           - Arguments:           - Options:</li> <li><code>deepbiop-cli fa-to-fq</code>           - Arguments:           - Options:</li> <li><code>deepbiop-cli fx-to-parquet</code>           - Arguments:           - Options:</li> <li><code>deepbiop-cli extract-fx</code>           - Arguments:           - Options:</li> <li><code>deepbiop-cli fxs-to-one</code>           - Arguments:           - Options:</li> <li><code>deepbiop-cli count-fx</code>           - Arguments:           - Options:</li> </ul>"},{"location":"cli/cli/#deepbiop-cli","title":"<code>deepbiop-cli</code>","text":"<p>CLI tool for Processing Biological Data.</p> <p>Usage: <code>deepbiop-cli [OPTIONS] [COMMAND]</code></p>"},{"location":"cli/cli/#subcommands","title":"Subcommands:","text":"<ul> <li><code>count-chimeric</code> \u2014 Count chimeric reads in a BAM file</li> <li><code>bam-to-fq</code> \u2014 BAM to fastq conversion</li> <li><code>fq-to-fa</code> \u2014 Fastq to fasta conversion</li> <li><code>fa-to-fq</code> \u2014 Fasta to fastq conversion</li> <li><code>fx-to-parquet</code> \u2014 Fastx to parquet conversion</li> <li><code>extract-fx</code> \u2014 Extract reads from a fastx file</li> <li><code>fxs-to-one</code> \u2014 Multiple Fastxs to one Fastx conversion</li> <li><code>count-fx</code> \u2014 Profile sequences in a fasta file</li> </ul>"},{"location":"cli/cli/#options","title":"Options:","text":"<ul> <li><code>--generate &lt;GENERATOR&gt;</code></li> </ul> <p>Possible values: <code>bash</code>, <code>elvish</code>, <code>fish</code>, <code>powershell</code>, <code>zsh</code></p> <ul> <li><code>-v</code>, <code>--verbose</code> \u2014 Increase logging verbosity</li> <li><code>-q</code>, <code>--quiet</code> \u2014 Decrease logging verbosity</li> </ul>"},{"location":"cli/cli/#deepbiop-cli-count-chimeric","title":"<code>deepbiop-cli count-chimeric</code>","text":"<p>Count chimeric reads in a BAM file</p> <p>Usage: <code>deepbiop-cli count-chimeric [OPTIONS] [bam]...</code></p>"},{"location":"cli/cli/#arguments","title":"Arguments:","text":"<ul> <li><code>&lt;bam&gt;</code> \u2014 path to the bam file</li> </ul>"},{"location":"cli/cli/#options_1","title":"Options:","text":"<ul> <li><code>-t</code>, <code>--threads &lt;THREADS&gt;</code> \u2014 threads number</li> </ul> <p>Default value: <code>2</code></p>"},{"location":"cli/cli/#deepbiop-cli-bam-to-fq","title":"<code>deepbiop-cli bam-to-fq</code>","text":"<p>BAM to fastq conversion</p> <p>Usage: <code>deepbiop-cli bam-to-fq [OPTIONS] [bam]...</code></p>"},{"location":"cli/cli/#arguments_1","title":"Arguments:","text":"<ul> <li><code>&lt;bam&gt;</code> \u2014 path to the bam file</li> </ul>"},{"location":"cli/cli/#options_2","title":"Options:","text":"<ul> <li><code>-t</code>, <code>--threads &lt;THREADS&gt;</code> \u2014 threads number</li> </ul> <p>Default value: <code>2</code></p> <ul> <li><code>-c</code>, <code>--compressed</code> \u2014 output bgzip compressed fastq file</li> </ul>"},{"location":"cli/cli/#deepbiop-cli-fq-to-fa","title":"<code>deepbiop-cli fq-to-fa</code>","text":"<p>Fastq to fasta conversion</p> <p>Usage: <code>deepbiop-cli fq-to-fa [OPTIONS] [fq]...</code></p>"},{"location":"cli/cli/#arguments_2","title":"Arguments:","text":"<ul> <li><code>&lt;fq&gt;</code> \u2014 path to the fq file</li> </ul>"},{"location":"cli/cli/#options_3","title":"Options:","text":"<ul> <li><code>-t</code>, <code>--threads &lt;THREADS&gt;</code> \u2014 threads number</li> </ul> <p>Default value: <code>2</code></p>"},{"location":"cli/cli/#deepbiop-cli-fa-to-fq","title":"<code>deepbiop-cli fa-to-fq</code>","text":"<p>Fasta to fastq conversion</p> <p>Usage: <code>deepbiop-cli fa-to-fq [OPTIONS] [fa]...</code></p>"},{"location":"cli/cli/#arguments_3","title":"Arguments:","text":"<ul> <li><code>&lt;fa&gt;</code> \u2014 path to the fa file</li> </ul>"},{"location":"cli/cli/#options_4","title":"Options:","text":"<ul> <li><code>-t</code>, <code>--threads &lt;THREADS&gt;</code> \u2014 threads number</li> </ul> <p>Default value: <code>2</code></p>"},{"location":"cli/cli/#deepbiop-cli-fx-to-parquet","title":"<code>deepbiop-cli fx-to-parquet</code>","text":"<p>Fastx to parquet conversion</p> <p>Usage: <code>deepbiop-cli fx-to-parquet [OPTIONS] &lt;fx&gt;</code></p>"},{"location":"cli/cli/#arguments_4","title":"Arguments:","text":"<ul> <li><code>&lt;fx&gt;</code> \u2014 path to the fx file</li> </ul>"},{"location":"cli/cli/#options_5","title":"Options:","text":"<ul> <li><code>--chunk</code> \u2014 if convert the fa file to parquet by chunk or not</li> <li><code>--chunk-size &lt;CHUNK_SIZE&gt;</code> \u2014 chunk size</li> </ul> <p>Default value: <code>1000000</code></p> <ul> <li><code>--output &lt;result&gt;</code> \u2014 result path</li> <li><code>-t</code>, <code>--threads &lt;THREADS&gt;</code> \u2014 threads number</li> </ul> <p>Default value: <code>2</code></p>"},{"location":"cli/cli/#deepbiop-cli-extract-fx","title":"<code>deepbiop-cli extract-fx</code>","text":"<p>Extract reads from a fastx file</p> <p>Usage: <code>deepbiop-cli extract-fx [OPTIONS] &lt;fx&gt;</code></p>"},{"location":"cli/cli/#arguments_5","title":"Arguments:","text":"<ul> <li><code>&lt;fx&gt;</code> \u2014 path to the fastx file</li> </ul>"},{"location":"cli/cli/#options_6","title":"Options:","text":"<ul> <li><code>--reads &lt;reads&gt;</code> \u2014 Path to the selected reads</li> <li><code>--number &lt;number&gt;</code> \u2014 The number of selected reads by random</li> <li><code>--output &lt;output&gt;</code> \u2014 output bgzip compressed file</li> <li><code>-t</code>, <code>--threads &lt;THREADS&gt;</code> \u2014 threads number</li> </ul> <p>Default value: <code>2</code></p> <ul> <li><code>-c</code>, <code>--compressed</code> \u2014 output bgzip compressed fasta file</li> </ul>"},{"location":"cli/cli/#deepbiop-cli-fxs-to-one","title":"<code>deepbiop-cli fxs-to-one</code>","text":"<p>Multiple Fastxs to one Fastx conversion</p> <p>Usage: <code>deepbiop-cli fxs-to-one [OPTIONS] --output &lt;output&gt; [fxs]...</code></p>"},{"location":"cli/cli/#arguments_6","title":"Arguments:","text":"<ul> <li><code>&lt;fxs&gt;</code> \u2014 path to the fx file</li> </ul>"},{"location":"cli/cli/#options_7","title":"Options:","text":"<ul> <li><code>--output &lt;output&gt;</code> \u2014 output bgzip compressed file</li> <li><code>-t</code>, <code>--threads &lt;THREADS&gt;</code></li> </ul> <p>Default value: <code>2</code></p>"},{"location":"cli/cli/#deepbiop-cli-count-fx","title":"<code>deepbiop-cli count-fx</code>","text":"<p>Profile sequences in a fasta file</p> <p>Usage: <code>deepbiop-cli count-fx [OPTIONS] &lt;fx&gt;</code></p>"},{"location":"cli/cli/#arguments_7","title":"Arguments:","text":"<ul> <li><code>&lt;fx&gt;</code> \u2014 path to the fastx file</li> </ul>"},{"location":"cli/cli/#options_8","title":"Options:","text":"<ul> <li><code>--export</code> \u2014 if export the result</li> <li><code>-t</code>, <code>--threads &lt;THREADS&gt;</code> \u2014 threads number</li> </ul> <p>Default value: <code>2</code></p> <p> This document was generated automatically by <code>clap-markdown</code>. </p>"},{"location":"development/versioning/","title":"Versioning","text":""},{"location":"development/versioning/#version-changes","title":"Version changes","text":"<p>Polars adheres to the semantic versioning specification:</p> <ul> <li>Breaking changes lead to a major version increase (<code>1.0.0</code>, <code>2.0.0</code>, ...)</li> <li>New features and performance improvements lead to a minor version increase (<code>1.1.0</code>, <code>1.2.0</code>, ...)</li> <li>Other changes lead to a patch version increase (<code>1.0.1</code>, <code>1.0.2</code>, ...)</li> </ul>"},{"location":"development/versioning/#policy-for-breaking-changes","title":"Policy for breaking changes","text":"<p>Polars takes backwards compatibility seriously, but we are not afraid to change things if it leads to a better product.</p>"},{"location":"development/versioning/#philosophy","title":"Philosophy","text":"<p>We don't always get it right on the first try. We learn as we go along and get feedback from our users. Sometimes, we're a little too eager to get out a new feature and didn't ponder all the possible implications.</p> <p>If this happens, we correct our mistakes and introduce a breaking change. Most of the time, this is no big deal. Users get a deprecation warning, they do a quick search-and-replace in their code base, and that's that.</p> <p>At times, we run into an issue requires more effort on our user's part to fix. A change in the query engine can seriously impact the assumptions in a data pipeline. We do not make such changes lightly, but we will make them if we believe it makes Polars better.</p> <p>Freeing ourselves of past indiscretions is important to keep Polars moving forward. We know it takes time and energy for our users to keep up with new releases but, in the end, it benefits everyone for Polars to be the best product possible.</p>"},{"location":"development/versioning/#what-qualifies-as-a-breaking-change","title":"What qualifies as a breaking change","text":"<p>A breaking change occurs when an existing component of the public API is changed or removed.</p> <p>A feature is part of the public API if it is documented in the API reference.</p> <p>Examples of breaking changes:</p> <ul> <li>A deprecated function or method is removed.</li> <li>The default value of a parameter is changed.</li> <li>The outcome of a query has changed due to changes to the query engine.</li> </ul> <p>Examples of changes that are not considered breaking:</p> <ul> <li>An undocumented function is removed.</li> <li>The module path of a public class is changed.</li> <li>An optional parameter is added to an existing method.</li> </ul> <p>Bug fixes are not considered a breaking change, even though it may impact some users' workflows.</p>"},{"location":"development/versioning/#unstable-functionality","title":"Unstable functionality","text":"<p>Some parts of the public API are marked as unstable. You can recognize this functionality from the warning in the API reference, or from the warning issued when the configuration option <code>warn_unstable</code> is active. There are a number of reasons functionality may be marked as unstable:</p> <ul> <li>We are unsure about the exact API. The name, function signature, or implementation are likely to change in the future.</li> <li>The functionality is not tested extensively yet. Bugs may pop up when used in real-world scenarios.</li> <li>The functionality does not yet integrate well with the full Polars API. You may find it works in one context but not in another.</li> </ul> <p>Releasing functionality as unstable allows us to gather important feedback from users that use Polars in real-world scenarios. This helps us fine-tune things before giving it our final stamp of approval. Users that are only interested in solid, well-tested functionality can avoid this part of the API.</p> <p>Functionality marked as unstable may change at any point without it being considered a breaking change.</p>"},{"location":"development/versioning/#deprecation-warnings","title":"Deprecation warnings","text":"<p>If we decide to introduce a breaking change, the existing behavior is deprecated if possible. For example, if we choose to rename a function, the new function is added alongside the old function, and using the old function will result in a deprecation warning.</p> <p>Not all changes can be deprecated nicely. A change to the query engine may have effects across a large part of the API. Such changes will not be warned for, but will be included in the changelog and the migration guide.</p> <p>Warning</p> <pre><code>Breaking changes to the Rust API are not deprecated first, but _will_ be listed in the changelog.\nSupporting deprecated functionality would slow down development too much at this point in time.\n</code></pre>"},{"location":"development/versioning/#deprecation-period","title":"Deprecation period","text":"<p>As a rule, deprecated functionality is removed two breaking releases after the deprecation happens. For example, a function deprecated in version <code>1.2.3</code> will be retained in version <code>2.0.0</code> and removed in version <code>3.0.0</code>.</p> <p>An exception to this rule are deprecations introduced with a breaking release. These will be enforced on the next breaking release. For example, a function deprecated in version <code>2.0.0</code> will be removed in version <code>3.0.0</code>.</p> <p>This means that if your program does not raise any deprecation warnings, it should be mostly safe to upgrade to the next major version. As breaking releases happen about once every six months, this allows six to twelve months to adjust to any pending breaking changes.</p> <p>In some cases, we may decide to adjust the deprecation period. If retaining the deprecated functionality blocks other improvements to Polars, we may shorten the deprecation period to a single breaking release. This will be mentioned in the warning message. If the deprecation affects many users, we may extend the deprecation period.</p>"},{"location":"development/versioning/#release-frequency","title":"Release frequency","text":"<p>Polars does not have a set release schedule. We issue a new release whenever we feel like we have something new and valuable to offer to our users. In practice, a new minor version is released about once every one or two weeks.</p>"},{"location":"development/versioning/#breaking-releases","title":"Breaking releases","text":"<p>Over time, issues pop up that require a breaking change to address. When enough issues have accumulated, we issue a breaking release.</p> <p>So far, breaking releases have happened about once every three to six months. The rate and severity of breaking changes will continue to diminish as Polars grows more solid. From this point on, we expect new major versions to be released about once every six months.</p>"},{"location":"development/contributing/","title":"Overview","text":"<p>Thanks for taking the time to contribute! We appreciate all contributions, from reporting bugs to implementing new features. If you're unclear on how to proceed after reading this guide, please contact us on Discord.</p>"},{"location":"development/contributing/#reporting-bugs","title":"Reporting bugs","text":"<p>We use GitHub issues to track bugs and suggested enhancements. You can report a bug by opening a new issue. Use the appropriate issue type for the language you are using (Rust / Python).</p> <p>Before creating a bug report, please check that your bug has not already been reported, and that your bug exists on the latest version of Polars. If you find a closed issue that seems to report the same bug you're experiencing, open a new issue and include a link to the original issue in your issue description.</p> <p>Please include as many details as possible in your bug report. The information helps the maintainers resolve the issue faster.</p>"},{"location":"development/contributing/#suggesting-enhancements","title":"Suggesting enhancements","text":"<p>We use GitHub issues to track bugs and suggested enhancements. You can suggest an enhancement by opening a new feature request. Before creating an enhancement suggestion, please check that a similar issue does not already exist.</p> <p>Please describe the behavior you want and why, and provide examples of how Polars would be used if your feature were added.</p>"},{"location":"development/contributing/#contributing-to-the-codebase","title":"Contributing to the codebase","text":""},{"location":"development/contributing/#picking-an-issue","title":"Picking an issue","text":"<p>Pick an issue by going through the issue tracker and finding an issue you would like to work on. Feel free to pick any issue with an accepted label that is not already assigned. We use the help wanted label to indicate issues that are high on our wishlist.</p> <p>If you are a first time contributor, you might want to look for issues labeled good first issue. The Polars code base is quite complex, so starting with a small issue will help you find your way around!</p> <p>If you would like to take on an issue, please comment on the issue to let others know. You may use the issue to discuss possible solutions.</p>"},{"location":"development/contributing/#setting-up-your-local-environment","title":"Setting up your local environment","text":"<p>The Polars development flow relies on both Rust and Python, which means setting up your local development environment is not trivial. If you run into problems, please contact us.</p> <p>Note</p> <pre><code>If you are a Windows user, the steps below might not work as expected.\nTry developing using [WSL](https://learn.microsoft.com/en-us/windows/wsl/install).\nUnder native Windows, you may have to manually copy the contents of `toolchain.toml` to `py-polars/toolchain.toml`, as Git for Windows may not correctly handle symbolic links.\n</code></pre>"},{"location":"development/contributing/#configuring-git","title":"Configuring Git","text":"<p>For contributing to Polars you need a free GitHub account and have git installed on your machine. Start by forking the Polars repository, then clone your forked repository using <code>git</code>:</p> <pre><code>git clone https://github.com/&lt;username&gt;/DeepBioP.git\ncd DeepBioP\n</code></pre> <p>Optionally set the <code>upstream</code> remote to be able to sync your fork with the Polars repository in the future:</p> <pre><code>git remote add upstream https://github.com/cauliyang/DeepBioP.git\ngit fetch upstream\n</code></pre>"},{"location":"development/contributing/#installing-dependencies","title":"Installing dependencies","text":"<p>In order to work on Polars effectively, you will need Rust, Python, and dprint.</p> <p>First, install Rust using rustup. After the initial installation, you will also need to install the nightly toolchain:</p> <pre><code>rustup toolchain install nightly --component miri\n</code></pre> <p>Next, install Python, for example using pyenv. We recommend using the latest Python version (<code>3.12</code>). Make sure you deactivate any active virtual environments (command: <code>deactivate</code>) or conda environments (command: <code>conda deactivate</code>), as the steps below will create a new virtual environment for Polars. You will need Python even if you intend to work on the Rust code only, as we rely on the Python tests to verify all functionality.</p> <p>Finally, install dprint. This is not strictly required, but it is recommended as we use it to autoformat certain file types.</p> <p>You can now check that everything works correctly by going into the <code>py-polars</code> directory and running the test suite (warning: this may be slow the first time you run it):</p> <pre><code>cd py-polars\nmake test\n</code></pre> <p>This will do a number of things:</p> <ul> <li>Use Python to create a virtual environment in the <code>.venv</code> folder.</li> <li>Use pip and uv to install all Python dependencies for development, linting, and building documentation.</li> <li>Use Rust to compile and install Polars in your virtual environment. At least 8GB of RAM is recommended for this step to run smoothly.</li> <li>Use pytest to run the Python unittests in your virtual environment</li> </ul> <p>Note</p> <pre><code>There are a small number of specialized dependencies that are not installed by default.\nIf you are running specific tests and encounter an error message about a missing dependency,\ntry running `make requirements-all` to install _all_ known dependencies).\n</code></pre> <p>Check if linting also works correctly by running:</p> <pre><code>make pre-commit\n</code></pre> <p>Note that we do not actually use the pre-commit tool. We use the Makefile to conveniently run the following formatting and linting tools:</p> <ul> <li>ruff</li> <li>mypy</li> <li>rustfmt</li> <li>clippy</li> <li>dprint</li> </ul> <p>If this all runs correctly, you're ready to start contributing to the Polars codebase!</p>"},{"location":"development/contributing/#updating-the-development-environment","title":"Updating the development environment","text":"<p>Dependencies are updated regularly - at least once per month. If you do not keep your environment up-to-date, you may notice tests or CI checks failing, or you may not be able to build Polars at all.</p> <p>To update your environment, first make sure your fork is in sync with the Polars repository:</p> <pre><code>git checkout main\ngit fetch upstream\ngit rebase upstream/main\ngit push origin main\n</code></pre> <p>Update all Python dependencies to their latest versions by running:</p> <pre><code>make requirements\n</code></pre> <p>If the Rust toolchain version has been updated, you should update your Rust toolchain. Follow it up by running <code>cargo clean</code> to make sure your Cargo folder does not grow too large:</p> <pre><code>rustup update\ncargo clean\n</code></pre>"},{"location":"development/contributing/#working-on-your-issue","title":"Working on your issue","text":"<p>Create a new git branch from the <code>main</code> branch in your local repository, and start coding!</p> <p>The Rust code is located in the <code>crates</code> directory, while the Python codebase is located in the <code>py-polars</code> directory. Both directories contain a <code>Makefile</code> with helpful commands. Most notably:</p> <ul> <li><code>make test</code> to run the test suite (see the test suite docs for more info)</li> <li><code>make pre-commit</code> to run autoformatting and linting</li> </ul> <p>Note that your work cannot be merged if these checks fail! Run <code>make help</code> to get a list of other helpful commands.</p> <p>Two other things to keep in mind:</p> <ul> <li>If you add code that should be tested, add tests.</li> <li>If you change the public API, update the documentation.</li> </ul>"},{"location":"development/contributing/#pull-requests","title":"Pull requests","text":"<p>When you have resolved your issue, open a pull request in the Polars repository. Please adhere to the following guidelines:</p> <ul> <li>Title</li> <li>Start your pull request title with a conventional commit tag.     This helps us add your contribution to the right section of the changelog.     We use the Angular convention.     Scope can be <code>rust</code> and/or <code>python</code>, depending on your contribution: this tag determines which changelog(s) will include your change.     Omit the scope if your change affects both Rust and Python.</li> <li>Use a descriptive title starting with an uppercase letter.     This text will end up in the changelog, so make sure the text is meaningful to the user.     Use single backticks to annotate code snippets.     Use active language and do not end your title with punctuation.</li> <li>Example: <code>fix(python): Fix `DataFrame.top_k` not handling nulls correctly</code></li> <li>Description</li> <li>In the pull request description, link to the issue you were working on.</li> <li>Add any relevant information to the description that you think may help the maintainers review your code.</li> <li>Make sure your branch is rebased against the latest version of the <code>main</code> branch.</li> <li>Make sure all GitHub Actions checks pass.</li> </ul> <p>After you have opened your pull request, a maintainer will review it and possibly leave some comments. Once all issues are resolved, the maintainer will merge your pull request, and your work will be part of the next Polars release!</p> <p>Keep in mind that your work does not have to be perfect right away! If you are stuck or unsure about your solution, feel free to open a draft pull request and ask for help.</p>"},{"location":"development/contributing/#contributing-to-documentation","title":"Contributing to documentation","text":"<p>The most important components of Polars documentation are the user guide, the API references, and the database of questions on StackOverflow.</p>"},{"location":"development/contributing/#user-guide","title":"User guide","text":"<p>The user guide is maintained in the <code>docs/user-guide</code> folder. Before creating a PR first raise an issue to discuss what you feel is missing or could be improved.</p>"},{"location":"development/contributing/#building-and-serving-the-user-guide","title":"Building and serving the user guide","text":"<p>The user guide is built using MkDocs. You install the dependencies for building the user guide by running <code>make build</code> in the root of the repo.</p> <p>Activate the virtual environment and run <code>mkdocs serve</code> to build and serve the user guide, so you can view it locally and see updates as you make changes.</p>"},{"location":"development/contributing/#creating-a-new-user-guide-page","title":"Creating a new user guide page","text":"<p>Each user guide page is based on a <code>.md</code> markdown file. This file must be listed in <code>mkdocs.yml</code>.</p>"},{"location":"development/contributing/#adding-a-shell-code-block","title":"Adding a shell code block","text":"<p>To add a code block with code to be run in a shell with tabs for Python and Rust, use the following format:</p> <pre><code>=== \":fontawesome-brands-python: Python\"\n\n    ```shell\n    $ pip install fsspec\n    ```\n\n=== \":fontawesome-brands-rust: Rust\"\n\n    ```shell\n    $ cargo add aws_sdk_s3\n    ```\n</code></pre>"},{"location":"development/contributing/#adding-a-code-block","title":"Adding a code block","text":"<p>The snippets for Python and Rust code blocks are in the <code>docs/src/python/</code> and <code>docs/src/rust/</code> directories, respectively. To add a code snippet with Python or Rust code to a <code>.md</code> page, use the following format:</p> <pre><code>{{code_block('user-guide/io/cloud-storage','read_parquet',['read_parquet','read_csv'])}}\n</code></pre> <ul> <li>The first argument is a path to either or both files called <code>docs/src/python/user-guide/io/cloud-storage.py</code> and <code>docs/src/rust/user-guide/io/cloud-storage.rs</code>.</li> <li>The second argument is the name given at the start and end of each snippet in the <code>.py</code> or <code>.rs</code> file</li> <li>The third argument is a list of links to functions in the API docs. For each element of the list there must be a corresponding entry in <code>docs/_build/API_REFERENCE_LINKS.yml</code></li> </ul> <p>If the corresponding <code>.py</code> and <code>.rs</code> snippet files both exist then each snippet named in the second argument to <code>code_block</code> above must exist or the build will fail. An empty snippet should be added to the <code>.py</code> or <code>.rs</code> file if the snippet is not needed.</p> <p>Each snippet is formatted as follows:</p> <pre><code># --8&lt;-- [start:read_parquet]\nimport polars as pl\n\ndf = pl.read_parquet(\"file.parquet\")\n# --8&lt;-- [end:read_parquet]\n</code></pre> <p>The snippet is delimited by <code>--8&lt;-- [start:&lt;snippet_name&gt;]</code> and <code>--8&lt;-- [end:&lt;snippet_name&gt;]</code>. The snippet name must match the name given in the second argument to <code>code_block</code> above.</p>"},{"location":"development/contributing/#linting","title":"Linting","text":"<p>Before committing, install <code>dprint</code> (see above) and run <code>dprint fmt</code> from the <code>docs</code> directory to lint the markdown files.</p>"},{"location":"development/contributing/#api-reference","title":"API reference","text":"<p>Polars has separate API references for Rust and Python. These are generated directly from the codebase, so in order to contribute, you will have to follow the steps outlined in this section above.</p>"},{"location":"development/contributing/#rust","title":"Rust","text":"<p>Rust Polars uses <code>cargo doc</code> to build its documentation. Contributions to improve or clarify the API reference are welcome.</p>"},{"location":"development/contributing/#python","title":"Python","text":"<p>For the Python API reference, we always welcome good docstring examples. There are still parts of the API that do not have any code examples. This is a great way to start contributing to Polars!</p> <p>Note that we follow the numpydoc convention. Docstring examples should also follow the Black codestyle. From the <code>py-polars</code> directory, run <code>make fmt</code> to make sure your additions pass the linter, and run <code>make doctest</code> to make sure your docstring examples are valid.</p> <p>Polars uses Sphinx to build the API reference. This means docstrings in general should follow the reST format. If you want to build the API reference locally, go to the <code>py-polars/docs</code> directory and run <code>make html</code>. The resulting HTML files will be in <code>py-polars/docs/build/html</code>.</p> <p>New additions to the API should be added manually to the API reference by adding an entry to the correct <code>.rst</code> file in the <code>py-polars/docs/source/reference</code> directory.</p>"},{"location":"development/contributing/#stackoverflow","title":"StackOverflow","text":"<p>We use StackOverflow to create a database of high quality questions and answers that is searchable and remains up-to-date. There is a separate tag for each language:</p> <ul> <li>Python Polars</li> <li>Rust Polars</li> </ul> <p>Contributions in the form of well-formulated questions or answers are always welcome! If you add a new question, please notify us by adding a matching issue to our GitHub issue tracker.</p>"},{"location":"development/contributing/#release-flow","title":"Release flow","text":"<p>This section is intended for Polars maintainers.</p> <p>Polars releases Rust crates to crates.io and Python packages to PyPI.</p> <p>New releases are marked by an official GitHub release and an associated git tag. We utilize Release Drafter to automatically draft GitHub releases with release notes.</p>"},{"location":"development/contributing/#steps","title":"Steps","text":"<p>The steps for releasing a new Rust or Python version are similar. The release process is mostly automated through GitHub Actions, but some manual steps are required. Follow the steps below to release a new version.</p> <p>Start by bumping the version number in the source code:</p> <ol> <li>Check the releases page on GitHub and find the appropriate draft release. Note the version number associated with this release.</li> <li>Make sure your fork is up-to-date with the latest version of the main Polars repository, and create a new branch.</li> <li> <p>Bump the version number.</p> </li> <li> <p>Rust: Update the version number in all <code>Cargo.toml</code> files in the <code>polars</code> directory and subdirectories. You'll probably want to use some search/replace strategy, as there are quite a few crates that need to be updated.</p> </li> <li> <p>Python: Update the version number in <code>py-polars/Cargo.toml</code> to match the version of the draft release.</p> </li> <li> <p>From the <code>py-polars</code> directory, run <code>make build</code> to generate a new <code>Cargo.lock</code> file.</p> </li> <li>Create a new commit with all files added. The name of the commit should follow the format <code>release(&lt;language&gt;): &lt;Language&gt; Polars &lt;version-number&gt;</code>. For example: <code>release(python): Python Polars 0.16.1</code></li> <li>Push your branch and open a new pull request to the <code>main</code> branch of the main Polars repository.</li> <li>Wait for the GitHub Actions checks to pass, then squash and merge your pull request.</li> </ol> <p>Directly after merging your pull request, release the new version:</p> <ol> <li>Go to the release workflow (Python/Rust), click Run workflow in the top right, and click the green button. This will trigger the workflow, which will build all release artifacts and publish them.</li> <li>Wait for the workflow to finish, then check crates.io/PyPI/GitHub to verify that the new Polars release is now available.</li> </ol>"},{"location":"development/contributing/#troubleshooting","title":"Troubleshooting","text":"<p>It may happen that one or multiple release jobs fail. If so, you should first try to simply re-run the failed jobs from the GitHub Actions UI.</p> <p>If that doesn't help, you will have to figure out what's wrong and commit a fix. Once your fix has made it to the <code>main</code> branch, simply re-trigger the release workflow.</p>"},{"location":"development/contributing/#license","title":"License","text":"<p>Any contributions you make to this project will fall under the MIT License that covers the Polars project.</p>"},{"location":"development/contributing/ci/","title":"Continuous integration","text":"<p>Polars uses GitHub Actions as its continuous integration (CI) tool. The setup is reasonably complex, as far as CI setups go. This page explains some of the design choices.</p>"},{"location":"development/contributing/ci/#goal","title":"Goal","text":"<p>Overall, the CI suite aims to achieve the following:</p> <ul> <li>Enforce code correctness by running automated tests.</li> <li>Enforce code quality by running automated linting checks.</li> <li>Enforce code performance by running benchmark tests.</li> <li>Enforce that code is properly documented.</li> <li>Allow maintainers to easily publish new releases.</li> </ul> <p>We rely on a wide range of tools to achieve this for both the Rust and the Python code base, and thus a lot of checks are triggered on each pull request.</p> <p>It's entirely possible that you submit a relatively trivial fix that subsequently fails a bunch of checks. Do not despair - check the logs to see what went wrong and try to fix it. You can run the failing command locally to verify that everything works correctly. If you can't figure it out, ask a maintainer for help!</p>"},{"location":"development/contributing/ci/#design","title":"Design","text":"<p>The CI setup is designed with the following requirements in mind:</p> <ul> <li>Get feedback on each step individually. We want to avoid our test job being cancelled because a linting check failed, only to find out later that we also have a failing test.</li> <li>Get feedback on each check as quickly as possible. We want to be able to iterate quickly if it turns out our code does not pass some of the checks.</li> <li>Only run checks when they need to be run. A change to the Rust code does not warrant a linting check of the Python code, for example.</li> </ul> <p>This results in a modular setup with many separate workflows and jobs that rely heavily on caching.</p>"},{"location":"development/contributing/ci/#modular-setup","title":"Modular setup","text":"<p>The repository consists of two main parts: the Rust code base and the Python code base. Both code bases are interdependent: Rust code is tested through Python tests, and the Python code relies on the Rust implementation for most functionality.</p> <p>To make sure CI jobs are only run when they need to be run, each workflow is triggered only when relevant files are modified.</p>"},{"location":"development/contributing/ci/#caching","title":"Caching","text":"<p>The main challenge is that the Rust code base for Polars is quite large, and consequently, compiling the project from scratch is slow. This is addressed by caching the Rust build artifacts.</p> <p>However, since GitHub Actions does not allow sharing caches between feature branches, we need to run the workflows on the main branch as well - at least the part that builds the Rust cache. This leads to many workflows that trigger both on pull request AND on push to the main branch, with individual steps of jobs enabled or disabled based on the branch it runs on.</p> <p>Care must also be taken not to exceed the maximum cache space of 10Gb allotted to open source GitHub repositories. Hence we do not do any caching on feature branches - we always use the cache available from the main branch. This also avoids any extra time that would be required to store the cache.</p>"},{"location":"development/contributing/ci/#releases","title":"Releases","text":"<p>The release jobs for Rust and Python are triggered manually. Refer to the contributing guide for the full release process.</p>"},{"location":"development/contributing/code-style/","title":"Code style","text":"<p>This page contains some guidance on code style.</p> <p>Info</p> <pre><code>Additional information will be added to this page later.\n</code></pre>"},{"location":"development/contributing/code-style/#rust","title":"Rust","text":""},{"location":"development/contributing/code-style/#naming-conventions","title":"Naming conventions","text":"<p>Naming conventions for variables:</p> <pre><code>let s: Series = ...\nlet ca: ChunkedArray = ...\nlet arr: ArrayRef = ...\nlet arr: PrimitiveArray = ...\nlet dtype: DataType = ...\nlet data_type: ArrowDataType = ...\n</code></pre>"},{"location":"development/contributing/code-style/#code-example","title":"Code example","text":"<pre><code>use std::{\n    fmt::Display,\n    path::{Path, PathBuf},\n    sync::Arc,\n};\n\nuse arrow::array::{Array, Int32Builder, ListBuilder, RecordBatch, StringBuilder};\nuse arrow::datatypes::{DataType, Field, Schema};\n\nuse bstr::BString;\nuse derive_builder::Builder;\nuse log::info;\nuse serde::{Deserialize, Serialize};\n\nuse crate::{io::write_parquet, types::Element};\n\nuse super::{triat::Encoder, FqEncoderOption, RecordData};\nuse anyhow::{Context, Result};\nuse pyo3::prelude::*;\nuse rayon::prelude::*;\n\n#[derive(Debug, Builder, Default)]\npub struct ParquetData {\n    pub id: BString,          // id\n    pub seq: BString,         // kmer_seq\n    pub qual: Vec&lt;Element&gt;,   // kmer_qual\n    pub target: Vec&lt;Element&gt;, // kmer_target\n}\n\n#[pyclass]\n#[derive(Debug, Builder, Default, Clone, Serialize, Deserialize)]\npub struct ParquetEncoder {\n    pub option: FqEncoderOption,\n}\n\nimpl ParquetEncoder {\n    pub fn new(option: FqEncoderOption) -&gt; Self {\n        Self { option }\n    }\n\n    fn generate_schema(&amp;self) -&gt; Arc&lt;Schema&gt; {\n        Arc::new(Schema::new(vec![\n            Field::new(\"id\", DataType::Utf8, false),\n            Field::new(\"seq\", DataType::Utf8, false),\n            Field::new(\n                \"qual\",\n                DataType::List(Box::new(Field::new(\"item\", DataType::Int32, true)).into()),\n                false,\n            ),\n            Field::new(\n                \"target\",\n                DataType::List(Box::new(Field::new(\"item\", DataType::Int32, true)).into()),\n                false,\n            ),\n        ]))\n    }\n\n    fn generate_batch(&amp;self, records: &amp;[RecordData], schema: &amp;Arc&lt;Schema&gt;) -&gt; Result&lt;RecordBatch&gt; {\n        let data: Vec&lt;ParquetData&gt; = records\n            .into_par_iter()\n            .filter_map(|data| {\n                let id = data.id.as_ref();\n                let seq = data.seq.as_ref();\n                let qual = data.qual.as_ref();\n                match self.encode_record(id, seq, qual).context(format!(\n                    \"encode fq read id {} error\",\n                    String::from_utf8_lossy(id)\n                )) {\n                    Ok(result) =&gt; Some(result),\n                    Err(_e) =&gt; None,\n                }\n            })\n            .collect();\n\n        // Create builders for each field\n        let mut id_builder = StringBuilder::new();\n        let mut seq_builder = StringBuilder::new();\n        let mut qual_builder = ListBuilder::new(Int32Builder::new());\n        let mut target_builder = ListBuilder::new(Int32Builder::new());\n\n        // Populate builders\n        data.into_iter().for_each(|parquet_record| {\n            id_builder.append_value(parquet_record.id.to_string());\n            seq_builder.append_value(parquet_record.seq.to_string());\n\n            parquet_record.qual.into_iter().for_each(|qual| {\n                qual_builder.values().append_value(qual);\n            });\n            qual_builder.append(true);\n\n            parquet_record.target.into_iter().for_each(|target| {\n                target_builder.values().append_value(target);\n            });\n            target_builder.append(true);\n        });\n\n        // Build arrays\n        let id_array = Arc::new(id_builder.finish());\n        let seq_array = Arc::new(seq_builder.finish());\n        let qual_array = Arc::new(qual_builder.finish());\n        let target_array = Arc::new(target_builder.finish());\n\n        // Create a RecordBatch\n        let record_batch = RecordBatch::try_new(\n            schema.clone(),\n            vec![\n                id_array as Arc&lt;dyn Array&gt;,\n                seq_array as Arc&lt;dyn Array&gt;,\n                qual_array as Arc&lt;dyn Array&gt;,\n                target_array as Arc&lt;dyn Array&gt;,\n            ],\n        )?;\n        Ok(record_batch)\n    }\n\n    pub fn encode_chunk&lt;P: AsRef&lt;Path&gt;&gt;(\n        &amp;mut self,\n        path: P,\n        chunk_size: usize,\n        parallel: bool,\n    ) -&gt; Result&lt;()&gt; {\n        let schema = self.generate_schema();\n        let records = self.fetch_records(&amp;path, self.option.kmer_size)?;\n        info!(\"Encoding records with chunk size {} \", chunk_size);\n\n        // create a folder for the chunk parquet files\n        let file_name = path.as_ref().file_name().unwrap().to_str().unwrap();\n        let chunks_folder = path\n            .as_ref()\n            .parent()\n            .unwrap()\n            .join(format!(\"{}_{}\", file_name, \"chunks\"))\n            .to_path_buf();\n        // create the folder\n        std::fs::create_dir_all(&amp;chunks_folder).context(\"Failed to create folder for chunks\")?;\n\n        if parallel {\n            records\n                // .chunks(chunk_size)\n                .par_chunks(chunk_size)\n                .enumerate()\n                .for_each(|(idx, record)| {\n                    let record_batch = self\n                        .generate_batch(record, &amp;schema)\n                        .context(format!(\"Failed to generate record batch for chunk {}\", idx))\n                        .unwrap();\n                    let parquet_path = chunks_folder.join(format!(\"{}_{}.parquet\", file_name, idx));\n                    write_parquet(parquet_path, record_batch, schema.clone())\n                        .context(format!(\"Failed to write parquet file for chunk {}\", idx))\n                        .unwrap();\n                });\n        } else {\n            records\n                .chunks(chunk_size)\n                .enumerate()\n                .for_each(|(idx, record)| {\n                    let record_batch = self\n                        .generate_batch(record, &amp;schema)\n                        .context(format!(\"Failed to generate record batch for chunk {}\", idx))\n                        .unwrap();\n                    let parquet_path = chunks_folder.join(format!(\"{}_{}.parquet\", file_name, idx));\n                    write_parquet(parquet_path, record_batch, schema.clone())\n                        .context(format!(\"Failed to write parquet file for chunk {}\", idx))\n                        .unwrap();\n                });\n        }\n\n        Ok(())\n    }\n}\n</code></pre>"},{"location":"development/contributing/ide/","title":"IDE configuration","text":"<p>Using an integrated development environments (IDE) and configuring it properly will help you work on Polars more effectively. This page contains some recommendations for configuring popular IDEs.</p>"},{"location":"development/contributing/ide/#visual-studio-code","title":"Visual Studio Code","text":"<p>Make sure to configure VSCode to use the virtual environment created by the Makefile.</p>"},{"location":"development/contributing/ide/#extensions","title":"Extensions","text":"<p>The extensions below are recommended.</p>"},{"location":"development/contributing/ide/#rust-analyzer","title":"rust-analyzer","text":"<p>If you work on the Rust code at all, you will need the rust-analyzer extension. This extension provides code completion for the Rust code.</p> <p>For it to work well for the Polars code base, add the following settings to your <code>.vscode/settings.json</code>:</p> <pre><code>{\n  \"rust-analyzer.cargo.features\": \"all\"\n}\n</code></pre>"},{"location":"development/contributing/ide/#ruff","title":"Ruff","text":"<p>The Ruff extension will help you conform to the formatting requirements of the Python code. We use both the Ruff linter and formatter. It is recommended to configure the extension to use the Ruff installed in your environment. This will make it use the correct Ruff version and configuration.</p> <pre><code>{\n  \"ruff.importStrategy\": \"fromEnvironment\"\n}\n</code></pre>"},{"location":"development/contributing/ide/#codelldb","title":"CodeLLDB","text":"<p>The CodeLLDB extension is useful for debugging Rust code. You can also debug Rust code called from Python (see section below).</p>"},{"location":"development/contributing/ide/#debugging","title":"Debugging","text":"<p>Due to the way that Python and Rust interoperate, debugging the Rust side of development from Python calls can be difficult. This guide shows how to set up a debugging environment that makes debugging Rust code called from a Python script painless.</p>"},{"location":"development/contributing/ide/#preparation","title":"Preparation","text":"<p>Start by installing the CodeLLDB extension (see above). Then add the following two configurations to your <code>launch.json</code> file. This file is usually found in the <code>.vscode</code> folder of your project root. See the official VSCode documentation for more information about the <code>launch.json</code> file.</p> <code>launch.json</code> <pre><code>{\n  \"configurations\": [\n    {\n      \"name\": \"Debug Rust/Python\",\n      \"type\": \"debugpy\",\n      \"request\": \"launch\",\n      \"program\": \"${workspaceFolder}/py-polars/debug/launch.py\",\n      \"args\": [\"${file}\"],\n      \"console\": \"internalConsole\",\n      \"justMyCode\": true,\n      \"serverReadyAction\": {\n        \"pattern\": \"pID = ([0-9]+)\",\n        \"action\": \"startDebugging\",\n        \"name\": \"Rust LLDB\"\n      }\n    },\n    {\n      \"name\": \"Rust LLDB\",\n      \"pid\": \"0\",\n      \"type\": \"lldb\",\n      \"request\": \"attach\",\n      \"program\": \"${workspaceFolder}/py-polars/.venv/bin/python\",\n      \"stopOnEntry\": false,\n      \"sourceLanguages\": [\"rust\"],\n      \"presentation\": {\n        \"hidden\": true\n      }\n    }\n  ]\n}\n</code></pre> <p>Info</p> <pre><code>On some systems, the LLDB debugger will not attach unless [ptrace protection](https://linux-audit.com/protect-ptrace-processes-kernel-yama-ptrace_scope) is disabled.\nTo disable, run the following command:\n\n```shell\necho 0 | sudo tee /proc/sys/kernel/yama/ptrace_scope\n```\n</code></pre>"},{"location":"development/contributing/ide/#running-the-debugger","title":"Running the debugger","text":"<ol> <li> <p>Create a Python script containing Polars code. Ensure that your virtual environment is activated.</p> </li> <li> <p>Set breakpoints in any <code>.rs</code> or <code>.py</code> file.</p> </li> <li> <p>In the <code>Run and Debug</code> panel on the left, select <code>Debug Rust/Python</code> from the drop-down menu on top and click the <code>Start Debugging</code> button.</p> </li> </ol> <p>At this point, your debugger should stop on breakpoints in any <code>.rs</code> file located within the codebase.</p>"},{"location":"development/contributing/ide/#details","title":"Details","text":"<p>The debugging feature runs via the specially-designed VSCode launch configuration shown above. The initial Python debugger is launched using a special launch script located at <code>py-polars/debug/launch.py</code> and passes the name of the script to be debugged (the target script) as an input argument. The launch script determines the process ID, writes this value into the <code>launch.json</code> configuration file, compiles the target script and runs it in the current environment. At this point, a second (Rust) debugger is attached to the Python debugger. The result is two simultaneous debuggers operating on the same running instance. Breakpoints in the Python code will stop on the Python debugger and breakpoints in the Rust code will stop on the Rust debugger.</p>"},{"location":"development/contributing/ide/#jetbrains-pycharm-rustrover-clion","title":"JetBrains (PyCharm, RustRover, CLion)","text":"<p>Info</p> <pre><code>More information needed.\n</code></pre>"},{"location":"development/contributing/test/","title":"Test suite","text":"<p>Info</p> <pre><code>Additional information on the Rust test suite will be added to this page later.\n</code></pre> <p>The <code>py-polars/tests</code> folder contains the main Polars test suite. This page contains some information on the various components of the test suite, as well as guidelines for writing new tests.</p> <p>The test suite contains four main components, each confined to their own folder: unit tests, parametric tests, benchmark tests, and doctests.</p> <p>Note that this test suite is indirectly responsible for testing Rust Polars as well. The Rust test suite is kept small to reduce compilation times. A lot of the Rust functionality is tested here instead.</p>"},{"location":"development/contributing/test/#unit-tests","title":"Unit tests","text":"<p>The <code>unit</code> folder contains all regular unit tests. These tests are intended to make sure all Polars functionality works as intended.</p>"},{"location":"development/contributing/test/#running-unit-tests","title":"Running unit tests","text":"<p>Run unit tests by running <code>make test</code> from the <code>py-polars</code> folder. This will compile the Rust bindings and then run the unit tests.</p> <p>If you're working in the Python code only, you can avoid recompiling every time by simply running <code>pytest</code> instead from your virtual environment.</p> <p>By default, \"slow\" tests and \"ci-only\" tests are skipped for local test runs. Such tests are marked using a custom pytest marker. To run these tests specifically, you can run <code>pytest -m slow</code>, <code>pytest -m ci_only</code>, <code>pytest -m slow ci_only</code> or run <code>pytest -m \"\"</code> to run all tests, regardless of marker.</p> <p>Note that the \"ci-only\" tests may require you to run <code>make requirements-all</code> to get additional dependencies (such as <code>torch</code>) that are otherwise not installed as part of the default Polars development environment.</p> <p>Tests can be run in parallel by running <code>pytest -n auto</code>. The parallelization is handled by <code>pytest-xdist</code>.</p>"},{"location":"development/contributing/test/#writing-unit-tests","title":"Writing unit tests","text":"<p>Whenever you add new functionality, you should also add matching unit tests. Add your tests to appropriate test module in the <code>unit</code> folder. Some guidelines to keep in mind:</p> <ul> <li>Try to fully cover all possible inputs and edge cases you can think of.</li> <li>Utilize pytest tools like <code>fixture</code> and <code>parametrize</code> where appropriate.</li> <li>Since many tests will require some data to be defined first, it can be efficient to run multiple checks in a single test. This can also be addressed using pytest fixtures.</li> <li>Unit tests should not depend on external factors, otherwise test parallelization will break.</li> </ul>"},{"location":"development/contributing/test/#parametric-tests","title":"Parametric tests","text":"<p>The <code>parametric</code> folder contains parametric tests written using the Hypothesis framework. These tests are intended to find and test edge cases by generating many random datapoints.</p>"},{"location":"development/contributing/test/#running-parametric-tests","title":"Running parametric tests","text":"<p>Run parametric tests by running <code>pytest -m hypothesis</code>.</p> <p>Note that parametric tests are excluded by default when running <code>pytest</code>. You must explicitly specify <code>-m hypothesis</code> to run them.</p> <p>These tests will be included when calculating test coverage, and will also be run as part of the <code>make test-all</code> make command.</p>"},{"location":"development/contributing/test/#doctests","title":"Doctests","text":"<p>The <code>docs</code> folder contains a script for running <code>doctest</code>. This folder does not contain any actual tests - rather, the script checks all docstrings in the Polars package for <code>Examples</code> sections, runs the code examples, and verifies the output.</p> <p>The aim of running <code>doctest</code> is to make sure the <code>Examples</code> sections in our docstrings are valid and remain up-to-date with code changes.</p>"},{"location":"development/contributing/test/#running-doctest","title":"Running <code>doctest</code>","text":"<p>To run the <code>doctest</code> module, run <code>make doctest</code> from the <code>py-polars</code> folder. You can also run the script directly from your virtual environment.</p> <p>Note that doctests are not run using pytest. While pytest does have the capability to run doc examples, configuration options are too limited for our purposes.</p> <p>Doctests will not count towards test coverage. They are not a substitute for unit tests, but rather intended to convey the intended use of the Polars API to the user.</p>"},{"location":"development/contributing/test/#writing-doc-examples","title":"Writing doc examples","text":"<p>Almost all classes/methods/functions that are part of Polars' public API should include code examples in their docstring. These examples help users understand basic usage and allow us to illustrate more advanced concepts as well. Some guidelines for writing a good docstring <code>Examples</code> section:</p> <ul> <li>Start with a minimal example that showcases the default functionality.</li> <li>Showcase the effect of its parameters.</li> <li>Showcase any special interactions when combined with other code.</li> <li>Keep it succinct and avoid multiple examples showcasing the same thing.</li> </ul> <p>There are many great docstring examples already, just check other code if you need inspiration!</p> <p>In addition to the regular options available when writing doctests, the script configuration allows for a new <code>IGNORE_RESULT</code> directive. Use this directive if you want to ensure the code runs, but the output may be random by design or not interesting to check.</p> <pre><code>&gt;&gt;&gt; df.sample(n=2)  # doctest: +IGNORE_RESULT\n</code></pre>"},{"location":"development/contributing/test/#benchmark-tests","title":"Benchmark tests","text":"<p>The <code>benchmark</code> folder contains code for running various benchmark tests. The aim of this part of the test suite is to spot performance regressions in the code, and to verify that Polars functionality works as expected when run on a release build or at a larger scale.</p> <p>Polars uses CodSpeed for tracking the performance of the benchmark tests.</p>"},{"location":"development/contributing/test/#generating-data","title":"Generating data","text":"<p>For most tests, a relatively large dataset must be generated first. This is done as part of the <code>pytest</code> setup process.</p> <p>The data generation logic was taken from the H2O.ai database benchmark, which is the foundation for many of the benchmark tests.</p>"},{"location":"development/contributing/test/#running-the-benchmark-tests","title":"Running the benchmark tests","text":"<p>The benchmark tests can be run using pytest. Run <code>pytest -m benchmark --durations 0 -v</code> to run these tests and report run duration.</p> <p>Note that benchmark tests are excluded by default when running <code>pytest</code>. You must explicitly specify <code>-m benchmark</code> to run them. They will also be excluded when calculating test coverage.</p> <p>These tests will be run as part of the <code>make test-all</code> make command.</p>"},{"location":"user-guide/getting-started/","title":"Getting started","text":"<p>This chapter is here to help you get started with DeepBiop. It covers all the fundamental features and functionalities of the library, making it easy for new users to familiarise themselves with the basics from initial installation and setup to core functionalities. If you're already an advanced user or familiar with Dataframes, feel free to skip ahead to the next chapter about installation options.</p>"},{"location":"user-guide/getting-started/#installing-deepbiop","title":"Installing deepbiop","text":"Python Rust <pre><code>pip install deepbiop\n</code></pre> <pre><code>cargo add deepbiop -F fastq\n\n# Or Cargo.toml\n[dependencies]\ndeepbiop = { version = \"x\", features = [\"fastq\", ...]}\n</code></pre>"},{"location":"user-guide/installation/","title":"Installation","text":"<p>Deepbiop is a library and installation is as simple as invoking the package manager of the corresponding programming language.</p>  Python Rust <pre><code>pip install deepbiop\n</code></pre> <pre><code>cargo add polars -F fastq\n\n# Or Cargo.toml\n[dependencies]\ndeepbiop = { version = \"x\", features = [\"fastq\", ...]}\n</code></pre>"},{"location":"user-guide/installation/#importing","title":"Importing","text":"<p>To use the library import it into your project</p>  Python Rust <pre><code>import deepbiop as dp\n</code></pre> <pre><code>use deepbiop::fastq;\n</code></pre>"}]}