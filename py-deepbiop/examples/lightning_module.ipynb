{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Lightning Integration with DeepBioP\n",
    "\n",
    "This notebook demonstrates how to use DeepBioP's BiologicalDataModule with PyTorch Lightning for streamlined deep learning workflows.\n",
    "\n",
    "## Features Demonstrated\n",
    "- Using BiologicalDataModule for train/val/test splits\n",
    "- Integration with Lightning Trainer\n",
    "- Automatic file type detection\n",
    "- Multi-GPU training support\n",
    "- Best practices for biological data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from deepbiop.lightning import BiologicalDataModule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic BiologicalDataModule Setup\n",
    "\n",
    "The BiologicalDataModule handles train/val/test splits and creates DataLoaders automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data module with train/val/test splits\n",
    "data_module = BiologicalDataModule(\n",
    "    train_path=\"../tests/data/test.fastq\",\n",
    "    val_path=\"../tests/data/test.fastq\",  # Using same file for demo\n",
    "    test_path=\"../tests/data/test.fastq\",  # Using same file for demo\n",
    "    batch_size=8,\n",
    "    num_workers=2,\n",
    ")\n",
    "\n",
    "# Setup creates the datasets\n",
    "data_module.setup(stage=\"fit\")\n",
    "\n",
    "# Access dataloaders\n",
    "train_loader = data_module.train_dataloader()\n",
    "val_loader = data_module.val_dataloader()\n",
    "\n",
    "print(f\"Train loader: {len(train_loader)} batches\")\n",
    "print(f\"Val loader: {len(val_loader)} batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Automatic File Type Detection\n",
    "\n",
    "BiologicalDataModule automatically detects file types from extensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FASTQ files (.fastq, .fq, .fastq.gz)\n",
    "fastq_module = BiologicalDataModule(train_path=\"../tests/data/test.fastq\", batch_size=4)\n",
    "\n",
    "# FASTA files (.fasta, .fa, .fasta.gz)\n",
    "fasta_module = BiologicalDataModule(train_path=\"../tests/data/test.fasta\", batch_size=4)\n",
    "\n",
    "# BAM files (.bam)\n",
    "bam_module = BiologicalDataModule(train_path=\"../tests/data/test.bam\", batch_size=4)\n",
    "\n",
    "print(\"Data modules created with automatic file type detection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating a Lightning Module\n",
    "\n",
    "Define a LightningModule for your model. Here's a simple sequence classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceClassifier(pl.LightningModule):\n",
    "    \"\"\"Simple CNN-based sequence classifier.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, vocab_size=256, embed_dim=32, num_classes=2, learning_rate=0.001\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Model layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.conv1 = nn.Conv1d(embed_dim, 64, kernel_size=7, padding=3)\n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=5, padding=2)\n",
    "        self.pool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.fc = nn.Linear(128, num_classes)\n",
    "\n",
    "        # Loss function\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, sequences):\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        x = self.embedding(sequences)  # (batch, seq_len, embed_dim)\n",
    "        x = x.transpose(1, 2)  # (batch, embed_dim, seq_len)\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.pool(x).squeeze(2)  # (batch, 128)\n",
    "        return self.fc(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"Training step.\"\"\"\n",
    "        # batch is a list of dicts from our dataset\n",
    "        # Convert to tensors (in practice, use a custom collate_fn)\n",
    "        sequences = torch.stack(\n",
    "            [torch.from_numpy(item[\"sequence\"]).long() for item in batch]\n",
    "        )\n",
    "\n",
    "        # Create dummy labels for demonstration\n",
    "        labels = torch.randint(0, 2, (len(batch),))\n",
    "\n",
    "        # Forward pass\n",
    "        logits = self(sequences)\n",
    "        loss = self.criterion(logits, labels)\n",
    "\n",
    "        # Log metrics\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \"\"\"Validation step.\"\"\"\n",
    "        sequences = torch.stack(\n",
    "            [torch.from_numpy(item[\"sequence\"]).long() for item in batch]\n",
    "        )\n",
    "        labels = torch.randint(0, 2, (len(batch),))\n",
    "\n",
    "        logits = self(sequences)\n",
    "        loss = self.criterion(logits, labels)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = (preds == labels).float().mean()\n",
    "\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log(\"val_acc\", acc, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        \"\"\"Test step.\"\"\"\n",
    "        sequences = torch.stack(\n",
    "            [torch.from_numpy(item[\"sequence\"]).long() for item in batch]\n",
    "        )\n",
    "        labels = torch.randint(0, 2, (len(batch),))\n",
    "\n",
    "        logits = self(sequences)\n",
    "        loss = self.criterion(logits, labels)\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = (preds == labels).float().mean()\n",
    "\n",
    "        self.log(\"test_loss\", loss)\n",
    "        self.log(\"test_acc\", acc)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Configure optimizer.\"\"\"\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training with Lightning Trainer\n",
    "\n",
    "Use the Lightning Trainer to train your model with the BiologicalDataModule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = SequenceClassifier(\n",
    "    vocab_size=256, embed_dim=32, num_classes=2, learning_rate=0.001\n",
    ")\n",
    "\n",
    "# Create data module\n",
    "data_module = BiologicalDataModule(\n",
    "    train_path=\"../tests/data/test.fastq\",\n",
    "    val_path=\"../tests/data/test.fastq\",\n",
    "    batch_size=8,\n",
    "    num_workers=2,\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=3,\n",
    "    accelerator=\"auto\",  # Automatically use GPU if available\n",
    "    devices=1,\n",
    "    enable_progress_bar=True,\n",
    "    enable_checkpointing=True,\n",
    "    default_root_dir=\"./lightning_logs\",\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Testing the Model\n",
    "\n",
    "After training, evaluate on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup test data\n",
    "data_module.setup(stage=\"test\")\n",
    "\n",
    "# Test the model\n",
    "results = trainer.test(model, data_module)\n",
    "print(\"Test results:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced: Custom Collate Function\n",
    "\n",
    "For production use, create a custom DataModule with a collate function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class CustomBioDataModule(BiologicalDataModule):\n",
    "    \"\"\"Custom DataModule with padding collate function.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        \"\"\"Custom collate with padding.\"\"\"\n",
    "        sequences = [torch.from_numpy(item[\"sequence\"]).long() for item in batch]\n",
    "\n",
    "        # Pad to max length\n",
    "        max_len = max(seq.shape[0] for seq in sequences)\n",
    "        padded = torch.zeros(len(sequences), max_len, dtype=torch.long)\n",
    "\n",
    "        for i, seq in enumerate(sequences):\n",
    "            padded[i, : seq.shape[0]] = seq\n",
    "\n",
    "        # Create dummy labels\n",
    "        labels = torch.randint(0, 2, (len(batch),))\n",
    "\n",
    "        return {\n",
    "            \"sequences\": padded,\n",
    "            \"labels\": labels,\n",
    "            \"lengths\": torch.tensor([seq.shape[0] for seq in sequences]),\n",
    "        }\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        \"\"\"Override to use custom collate.\"\"\"\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            collate_fn=self.collate_fn,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        \"\"\"Override to use custom collate.\"\"\"\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            collate_fn=self.collate_fn,\n",
    "        )\n",
    "\n",
    "\n",
    "# Use custom data module\n",
    "custom_data_module = CustomBioDataModule(\n",
    "    train_path=\"../tests/data/test.fastq\",\n",
    "    val_path=\"../tests/data/test.fastq\",\n",
    "    batch_size=8,\n",
    "    num_workers=2,\n",
    ")\n",
    "\n",
    "print(\"Custom DataModule with padding created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Multi-GPU Training\n",
    "\n",
    "Lightning makes multi-GPU training easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-GPU trainer\n",
    "multi_gpu_trainer = pl.Trainer(\n",
    "    max_epochs=3,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=2,  # Use 2 GPUs\n",
    "    strategy=\"ddp\",  # Distributed Data Parallel\n",
    "    enable_progress_bar=True,\n",
    ")\n",
    "\n",
    "# Note: This will only work if you have multiple GPUs\n",
    "# The DataModule handles distributed data loading automatically\n",
    "print(\"Multi-GPU trainer configured (requires multiple GPUs)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Logging and Checkpointing\n",
    "\n",
    "Lightning provides built-in logging and checkpointing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "# Create logger\n",
    "logger = TensorBoardLogger(save_dir=\"./logs\", name=\"sequence_classifier\")\n",
    "\n",
    "# Create callbacks\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    dirpath=\"./checkpoints\",\n",
    "    filename=\"best-{epoch:02d}-{val_loss:.2f}\",\n",
    "    save_top_k=3,\n",
    "    mode=\"min\",\n",
    ")\n",
    "\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", patience=3, mode=\"min\")\n",
    "\n",
    "# Trainer with callbacks and logger\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10,\n",
    "    callbacks=[checkpoint_callback, early_stop_callback],\n",
    "    logger=logger,\n",
    "    enable_progress_bar=True,\n",
    ")\n",
    "\n",
    "print(\"Trainer with logging and checkpointing configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "- ✅ BiologicalDataModule for train/val/test splits\n",
    "- ✅ Automatic file type detection (FASTQ, FASTA, BAM)\n",
    "- ✅ Creating Lightning modules for biological data\n",
    "- ✅ Training and testing with Lightning Trainer\n",
    "- ✅ Custom collate functions for padding\n",
    "- ✅ Multi-GPU training setup\n",
    "- ✅ Logging and checkpointing\n",
    "\n",
    "BiologicalDataModule simplifies deep learning on biological data by handling:\n",
    "- Data loading from multiple file formats\n",
    "- Train/val/test splitting\n",
    "- DataLoader configuration\n",
    "- Integration with Lightning's ecosystem\n",
    "\n",
    "This allows you to focus on model development rather than data engineering!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
