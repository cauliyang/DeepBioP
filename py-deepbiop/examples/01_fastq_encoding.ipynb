{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FASTQ Loading and Sequence Encoding\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Load FASTQ files\n",
    "2. Encode sequences for machine learning (one-hot, k-mer, integer)\n",
    "3. Export to NumPy arrays for ML frameworks\n",
    "\n",
    "## Installation\n",
    "\n",
    "```bash\n",
    "pip install deepbiop\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import deepbiop as dbp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. One-Hot Encoding for CNNs/RNNs\n",
    "\n",
    "One-hot encoding represents each base as a binary vector:\n",
    "- A = [1, 0, 0, 0]\n",
    "- C = [0, 1, 0, 0]\n",
    "- G = [0, 0, 1, 0]\n",
    "- T = [0, 0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create encoder\n",
    "encoder = dbp.OneHotEncoder(\"dna\", \"skip\")\n",
    "\n",
    "# Example sequences\n",
    "sequences = [b\"ACGTACGT\", b\"TTGGCCAA\", b\"AAAACCCC\"]\n",
    "\n",
    "# Encode batch\n",
    "encoded = encoder.encode_batch(sequences)\n",
    "\n",
    "print(f\"Encoded shape: {encoded.shape}\")  # (3, 8, 4) = [batch, seq_len, alphabet_size]\n",
    "print(f\"First sequence encoded:\\n{encoded[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize one-hot encoding\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.imshow(encoded[0].T, cmap=\"YlGnBu\", aspect=\"auto\")\n",
    "plt.yticks([0, 1, 2, 3], [\"A\", \"C\", \"G\", \"T\"])\n",
    "plt.xlabel(\"Position\")\n",
    "plt.ylabel(\"Base\")\n",
    "plt.title(\"One-Hot Encoding: ACGTACGT\")\n",
    "plt.colorbar(label=\"Value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. K-mer Encoding for Feature-Based Models\n",
    "\n",
    "K-mer encoding counts overlapping k-length subsequences.\n",
    "Useful for traditional ML models (Random Forest, SVM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create k-mer encoder (k=3, canonical)\n",
    "kmer_encoder = dbp.KmerEncoder(k=3, canonical=True, encoding_type=\"dna\")\n",
    "\n",
    "# Encode sequences\n",
    "kmer_features = kmer_encoder.encode_batch(sequences)\n",
    "\n",
    "print(f\"K-mer features shape: {kmer_features.shape}\")  # (3, 64) for 4^3 possible 3-mers\n",
    "print(\n",
    "    f\"Number of unique 3-mers in first sequence: {np.count_nonzero(kmer_features[0])}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize k-mer counts\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.bar(range(len(kmer_features[0])), kmer_features[0])\n",
    "plt.xlabel(\"K-mer Index\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"3-mer Frequency Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Integer Encoding for Transformers/Embeddings\n",
    "\n",
    "Integer encoding maps each base to an integer:\n",
    "- A = 0, C = 1, G = 2, T = 3\n",
    "\n",
    "Ideal for transformer models and embedding layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create integer encoder\n",
    "int_encoder = dbp.IntegerEncoder(\"dna\")\n",
    "\n",
    "# Encode sequences\n",
    "int_encoded = int_encoder.encode_batch(sequences)\n",
    "\n",
    "print(f\"Integer encoded shape: {int_encoded.shape}\")  # (3, 8)\n",
    "print(f\"First sequence: {sequences[0]}\")\n",
    "print(f\"Encoded: {int_encoded[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Integration with PyTorch\n",
    "\n",
    "DeepBioP outputs are NumPy arrays that work seamlessly with PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "tensor_onehot = torch.from_numpy(encoded).float()\n",
    "tensor_int = torch.from_numpy(int_encoded).long()\n",
    "\n",
    "print(f\"PyTorch one-hot tensor shape: {tensor_onehot.shape}\")\n",
    "print(f\"PyTorch integer tensor shape: {tensor_int.shape}\")\n",
    "print(f\"PyTorch tensor dtype: {tensor_int.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Integration with HuggingFace Transformers\n",
    "\n",
    "Use integer encoding with special tokens for transformer models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add special tokens (CLS=4, SEP=5, PAD=6)\n",
    "input_ids = torch.from_numpy(int_encoded).long() + 7  # Offset for special tokens\n",
    "\n",
    "# Create attention mask (1 for real tokens, 0 for padding)\n",
    "attention_mask = (input_ids != 6).long()\n",
    "\n",
    "print(f\"Input IDs: {input_ids[0]}\")\n",
    "print(f\"Attention mask: {attention_mask[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Export to Files\n",
    "\n",
    "Save encoded sequences for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to NumPy files\n",
    "dbp.utils.export_to_numpy_int(\"sequences_int.npy\", sequences)\n",
    "dbp.utils.export_to_numpy_onehot(\"sequences_onehot.npy\", sequences)\n",
    "\n",
    "# Load back\n",
    "loaded_int = np.load(\"sequences_int.npy\")\n",
    "loaded_onehot = np.load(\"sequences_onehot.npy\")\n",
    "\n",
    "print(f\"Loaded integer encoding: {loaded_int.shape}\")\n",
    "print(f\"Loaded one-hot encoding: {loaded_onehot.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "DeepBioP provides three encoding schemes optimized for different ML architectures:\n",
    "\n",
    "| Encoding | Shape | Use Case | Models |\n",
    "|----------|-------|----------|--------|\n",
    "| One-Hot | (N, L, 4) | Spatial features | CNN, RNN, LSTM |\n",
    "| K-mer | (N, 4^k) | Feature vectors | Random Forest, SVM |\n",
    "| Integer | (N, L) | Token sequences | Transformers, Embeddings |\n",
    "\n",
    "All encodings:\n",
    "- ✅ Zero-copy NumPy arrays\n",
    "- ✅ Batch processing support\n",
    "- ✅ PyTorch/TensorFlow compatible\n",
    "- ✅ HuggingFace Transformers ready"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
