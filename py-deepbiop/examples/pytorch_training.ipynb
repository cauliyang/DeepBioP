{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch DataLoader Integration with DeepBioP\n",
    "\n",
    "This notebook demonstrates how to use DeepBioP's streaming datasets with PyTorch's DataLoader for efficient training on biological sequence data.\n",
    "\n",
    "## Features Demonstrated\n",
    "- Loading FASTQ, FASTA, and BAM files with streaming datasets\n",
    "- Using PyTorch DataLoader for batching\n",
    "- Multiprocessing with `num_workers > 0`\n",
    "- Custom collate functions for biological data\n",
    "- Distributed training with DistributedSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic DataLoader Usage with FASTQ Files\n",
    "\n",
    "The simplest way to use DeepBioP datasets with PyTorch is to create a streaming dataset and wrap it in a DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepbiop.fq import FastqStreamDataset\n",
    "\n",
    "# Create a streaming dataset\n",
    "dataset = FastqStreamDataset(\"../tests/data/test.fastq\")\n",
    "\n",
    "# Wrap in DataLoader\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=4,\n",
    "    num_workers=0,  # Single-threaded for now\n",
    "    shuffle=False,  # Streaming datasets don't support shuffle\n",
    ")\n",
    "\n",
    "# Iterate through batches\n",
    "for batch_idx, batch in enumerate(loader):\n",
    "    print(f\"Batch {batch_idx}:\")\n",
    "    print(f\"  - Number of records: {len(batch)}\")\n",
    "\n",
    "    # Each batch is a list of dicts\n",
    "    for item in batch:\n",
    "        print(f\"    ID: {item['id']}\")\n",
    "        print(f\"    Sequence shape: {item['sequence'].shape}\")\n",
    "        print(\n",
    "            f\"    Quality shape: {item['quality'].shape if item['quality'] is not None else 'None'}\"\n",
    "        )\n",
    "\n",
    "    if batch_idx >= 2:  # Only show first 3 batches\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Multiprocessing with DataLoader\n",
    "\n",
    "For faster data loading, you can use multiple worker processes. DeepBioP datasets support pickling for multiprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create loader with multiple workers\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=8,\n",
    "    num_workers=4,  # 4 worker processes\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "# Count total records loaded\n",
    "total_records = sum(len(batch) for batch in loader)\n",
    "print(f\"Total records loaded with multiprocessing: {total_records}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Custom Collate Function for Sequence Data\n",
    "\n",
    "Often you need to transform the data before batching. Here's an example collate function that converts sequences to tensors and pads them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bio_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function for biological sequences.\n",
    "\n",
    "    Converts sequences to tensors and pads to max length in batch.\n",
    "    \"\"\"\n",
    "    # Extract sequences and quality scores\n",
    "    sequences = [torch.from_numpy(item[\"sequence\"]).long() for item in batch]\n",
    "    qualities = [\n",
    "        torch.from_numpy(item[\"quality\"]).long()\n",
    "        if item[\"quality\"] is not None\n",
    "        else None\n",
    "        for item in batch\n",
    "    ]\n",
    "    ids = [item[\"id\"] for item in batch]\n",
    "\n",
    "    # Pad sequences to max length in batch\n",
    "    max_len = max(seq.shape[0] for seq in sequences)\n",
    "\n",
    "    padded_seqs = torch.zeros(len(sequences), max_len, dtype=torch.long)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        padded_seqs[i, : seq.shape[0]] = seq\n",
    "\n",
    "    # Pad quality scores similarly\n",
    "    if qualities[0] is not None:\n",
    "        padded_quals = torch.zeros(len(qualities), max_len, dtype=torch.long)\n",
    "        for i, qual in enumerate(qualities):\n",
    "            if qual is not None:\n",
    "                padded_quals[i, : qual.shape[0]] = qual\n",
    "    else:\n",
    "        padded_quals = None\n",
    "\n",
    "    return {\n",
    "        \"sequences\": padded_seqs,\n",
    "        \"qualities\": padded_quals,\n",
    "        \"ids\": ids,\n",
    "        \"lengths\": torch.tensor([seq.shape[0] for seq in sequences]),\n",
    "    }\n",
    "\n",
    "\n",
    "# Use custom collate function\n",
    "loader = DataLoader(dataset, batch_size=4, collate_fn=bio_collate_fn, num_workers=2)\n",
    "\n",
    "# Check output format\n",
    "batch = next(iter(loader))\n",
    "print(f\"Batch keys: {batch.keys()}\")\n",
    "print(f\"Sequences shape: {batch['sequences'].shape}\")\n",
    "print(f\"Lengths: {batch['lengths']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Loop Example\n",
    "\n",
    "Here's a complete example of a training loop with a simple sequence classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class SequenceClassifier(nn.Module):\n",
    "    \"\"\"Simple CNN for sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size=256, embed_dim=32, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.conv1 = nn.Conv1d(embed_dim, 64, kernel_size=7, padding=3)\n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=5, padding=2)\n",
    "        self.pool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.fc = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch, seq_len) containing sequence indices.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            Output tensor of shape (batch, num_classes) containing class logits.\n",
    "        \"\"\"\n",
    "        # x: (batch, seq_len)\n",
    "        x = self.embedding(x)  # (batch, seq_len, embed_dim)\n",
    "        x = x.transpose(1, 2)  # (batch, embed_dim, seq_len)\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.pool(x).squeeze(2)  # (batch, 128)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "# Create model and optimizer\n",
    "model = SequenceClassifier()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "num_epochs = 2\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    for batch in loader:\n",
    "        sequences = batch[\"sequences\"]\n",
    "\n",
    "        # Create dummy labels for demonstration\n",
    "        labels = torch.randint(0, 2, (sequences.size(0),))\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(sequences)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "        if num_batches >= 10:  # Limit for demo\n",
    "            break\n",
    "\n",
    "    avg_loss = total_loss / num_batches\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Using FASTA and BAM Files\n",
    "\n",
    "The same DataLoader pattern works with FASTA and BAM files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepbiop.bam import BamStreamDataset\n",
    "from deepbiop.fa import FastaStreamDataset\n",
    "\n",
    "# FASTA example\n",
    "fasta_dataset = FastaStreamDataset(\"../tests/data/test.fasta\")\n",
    "fasta_loader = DataLoader(fasta_dataset, batch_size=4)\n",
    "\n",
    "print(\"FASTA samples:\")\n",
    "for batch in fasta_loader:\n",
    "    print(f\"  Batch size: {len(batch)}\")\n",
    "    print(f\"  First ID: {batch[0]['id']}\")\n",
    "    break\n",
    "\n",
    "# BAM example (with threading support)\n",
    "bam_dataset = BamStreamDataset(\"../tests/data/test.bam\", threads=4)\n",
    "bam_loader = DataLoader(bam_dataset, batch_size=4)\n",
    "\n",
    "print(\"\\nBAM samples:\")\n",
    "for batch in bam_loader:\n",
    "    print(f\"  Batch size: {len(batch)}\")\n",
    "    print(f\"  First ID: {batch[0]['id']}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Worker Init Function for Reproducibility\n",
    "\n",
    "For reproducible experiments with multiprocessing, use `worker_init_fn` to set seeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def worker_init_fn(worker_id):\n",
    "    \"\"\"Initialize each worker with a unique but deterministic seed.\"\"\"\n",
    "    seed = 42 + worker_id\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=4, num_workers=4, worker_init_fn=worker_init_fn)\n",
    "\n",
    "print(\"DataLoader with deterministic worker initialization created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Distributed Training Setup\n",
    "\n",
    "For distributed training across multiple GPUs or nodes, use DistributedSampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DistributedSampler\n",
    "\n",
    "# Note: In actual distributed training, these would come from torch.distributed\n",
    "num_replicas = 2  # Number of GPUs/processes\n",
    "rank = 0  # Current process rank\n",
    "\n",
    "# Create distributed sampler\n",
    "sampler = DistributedSampler(\n",
    "    dataset,\n",
    "    num_replicas=num_replicas,\n",
    "    rank=rank,\n",
    "    shuffle=False,  # Streaming datasets handle their own iteration\n",
    ")\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=4,\n",
    "    sampler=sampler,  # Use sampler instead of shuffle\n",
    "    num_workers=2,\n",
    ")\n",
    "\n",
    "# Each process will see a subset of the data\n",
    "records_seen = sum(len(batch) for batch in loader)\n",
    "print(f\"Rank {rank} processed {records_seen} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "- ✅ Basic DataLoader usage with streaming datasets\n",
    "- ✅ Multiprocessing with `num_workers`\n",
    "- ✅ Custom collate functions for biological data\n",
    "- ✅ Complete training loop example\n",
    "- ✅ FASTA and BAM file support\n",
    "- ✅ Worker initialization for reproducibility\n",
    "- ✅ Distributed training setup\n",
    "\n",
    "DeepBioP's streaming datasets integrate seamlessly with PyTorch's DataLoader, providing efficient, memory-friendly iteration over large biological files."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
