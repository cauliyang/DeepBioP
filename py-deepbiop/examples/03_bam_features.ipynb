{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BAM/SAM Alignment Feature Extraction\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Load BAM files with multithreaded decompression\n",
    "2. Extract alignment features for machine learning\n",
    "3. Filter alignments by quality and pairing\n",
    "4. Count chimeric reads\n",
    "5. Export to Arrow/Parquet for analysis\n",
    "\n",
    "## Installation\n",
    "\n",
    "```bash\n",
    "pip install deepbiop\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import deepbiop as dbp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading BAM Files\n",
    "\n",
    "DeepBioP provides multithreaded BAM decompression for high performance.\n",
    "\n",
    "BAM (Binary Alignment/Map) format stores aligned sequencing reads:\n",
    "- Reference sequence alignment position\n",
    "- CIGAR string (alignment operations)\n",
    "- Mapping quality (MAPQ)\n",
    "- SAM flags (paired, mapped, supplementary, etc.)\n",
    "- Optional tags (edit distance, alignment score, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open BAM file with 4 decompression threads\n",
    "# Adjust thread count based on CPU cores\n",
    "reader = dbp.BamReader(\"alignments.bam\", threads=4)\n",
    "\n",
    "print(\"BAM reader created with multithreaded decompression\")\n",
    "print(\"Ready to extract alignment features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extracting Alignment Features\n",
    "\n",
    "AlignmentFeatures captures key quality metrics:\n",
    "\n",
    "| Feature | Description | ML Use Case |\n",
    "|---------|-------------|-------------|\n",
    "| mapping_quality | Phred-scaled confidence (0-255) | Filter low-quality alignments |\n",
    "| aligned_length | Total aligned bases | Normalize coverage |\n",
    "| num_matches | Matching bases | Calculate identity |\n",
    "| num_insertions | Insertion events | Detect indels |\n",
    "| num_deletions | Deletion events | Variant calling features |\n",
    "| num_soft_clips | Soft-clipped bases | Structural variant signals |\n",
    "| edit_distance | Total mismatches + indels | Alignment quality metric |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from all alignments\n",
    "features = reader.extract_features()\n",
    "\n",
    "print(f\"Extracted features from {len(features)} alignments\")\n",
    "print(\"\\nFirst 5 alignments:\")\n",
    "for i, feat in enumerate(features[:5], 1):\n",
    "    print(f\"\\nAlignment {i}:\")\n",
    "    print(f\"  Mapping quality: {feat.mapping_quality}\")\n",
    "    print(f\"  Aligned length: {feat.aligned_length}bp\")\n",
    "    print(f\"  Matches: {feat.num_matches}\")\n",
    "    print(f\"  Identity: {feat.identity():.3f}\")\n",
    "    print(f\"  Is mapped: {feat.is_mapped}\")\n",
    "    print(f\"  Is paired: {feat.is_paired}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Computing Feature Statistics\n",
    "\n",
    "### Identity\n",
    "Identity = matches / (matches + mismatches + insertions + deletions)\n",
    "\n",
    "High identity (>0.95) indicates good alignment quality.\n",
    "\n",
    "### Indel Rate\n",
    "Indel rate = (insertions + deletions) / aligned_length\n",
    "\n",
    "High indel rates may indicate sequencing errors or structural variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate statistics across all alignments\n",
    "identities = [feat.identity() for feat in features if feat.is_mapped]\n",
    "mapq_scores = [feat.mapping_quality for feat in features if feat.is_mapped]\n",
    "aligned_lengths = [feat.aligned_length for feat in features if feat.is_mapped]\n",
    "\n",
    "print(\"Alignment Statistics:\")\n",
    "print(f\"  Mean identity: {np.mean(identities):.3f}\")\n",
    "print(f\"  Median identity: {np.median(identities):.3f}\")\n",
    "print(f\"  Mean MAPQ: {np.mean(mapq_scores):.1f}\")\n",
    "print(f\"  Mean aligned length: {np.mean(aligned_lengths):.1f}bp\")\n",
    "print(f\"  Mapped reads: {sum(1 for f in features if f.is_mapped)} / {len(features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualizing Alignment Quality\n",
    "\n",
    "Quality distributions help identify:\n",
    "- Poor alignment regions (low MAPQ)\n",
    "- Mapping biases\n",
    "- Outlier samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Mapping quality distribution\n",
    "axes[0, 0].hist(mapq_scores, bins=50, color=\"skyblue\", edgecolor=\"black\")\n",
    "axes[0, 0].set_xlabel(\"Mapping Quality (MAPQ)\")\n",
    "axes[0, 0].set_ylabel(\"Count\")\n",
    "axes[0, 0].set_title(\"Mapping Quality Distribution\")\n",
    "axes[0, 0].axvline(x=30, color=\"r\", linestyle=\"--\", label=\"Q30 threshold\")\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Identity distribution\n",
    "axes[0, 1].hist(identities, bins=50, color=\"lightgreen\", edgecolor=\"black\")\n",
    "axes[0, 1].set_xlabel(\"Alignment Identity\")\n",
    "axes[0, 1].set_ylabel(\"Count\")\n",
    "axes[0, 1].set_title(\"Alignment Identity Distribution\")\n",
    "axes[0, 1].axvline(x=0.95, color=\"r\", linestyle=\"--\", label=\"95% threshold\")\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Aligned length distribution\n",
    "axes[1, 0].hist(aligned_lengths, bins=50, color=\"salmon\", edgecolor=\"black\")\n",
    "axes[1, 0].set_xlabel(\"Aligned Length (bp)\")\n",
    "axes[1, 0].set_ylabel(\"Count\")\n",
    "axes[1, 0].set_title(\"Aligned Length Distribution\")\n",
    "\n",
    "# MAPQ vs Identity scatter\n",
    "axes[1, 1].scatter(mapq_scores, identities, alpha=0.3, s=10)\n",
    "axes[1, 1].set_xlabel(\"Mapping Quality (MAPQ)\")\n",
    "axes[1, 1].set_ylabel(\"Alignment Identity\")\n",
    "axes[1, 1].set_title(\"MAPQ vs Identity\")\n",
    "axes[1, 1].axhline(y=0.95, color=\"r\", linestyle=\"--\", alpha=0.5)\n",
    "axes[1, 1].axvline(x=30, color=\"r\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Filtering High-Quality Alignments\n",
    "\n",
    "Common quality thresholds:\n",
    "- **MAPQ ≥ 30**: High-confidence unique mapping (error rate 0.001)\n",
    "- **MAPQ ≥ 20**: Good quality (error rate 0.01)\n",
    "- **MAPQ ≥ 10**: Moderate quality (error rate 0.1)\n",
    "- **MAPQ = 0**: Multi-mapping or unmapped\n",
    "\n",
    "For ML pipelines, typically use MAPQ ≥ 30 for training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by mapping quality (MAPQ >= 30)\n",
    "reader2 = dbp.BamReader(\"alignments.bam\", threads=4)\n",
    "high_qual_count = reader2.filter_by_mapping_quality(30)\n",
    "\n",
    "print(f\"High-quality alignments (MAPQ ≥ 30): {high_qual_count}\")\n",
    "print(f\"Percentage of total: {high_qual_count / len(features) * 100:.1f}%\")\n",
    "\n",
    "# Compare quality tiers\n",
    "thresholds = [0, 10, 20, 30, 40]\n",
    "counts = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    count = sum(1 for f in features if f.mapping_quality >= threshold)\n",
    "    counts.append(count)\n",
    "    print(f\"MAPQ ≥ {threshold:2d}: {count:6d} ({count / len(features) * 100:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Proper Pair Analysis\n",
    "\n",
    "For paired-end sequencing, proper pairs have:\n",
    "- Both reads mapped\n",
    "- Same reference sequence\n",
    "- Expected orientation (forward-reverse)\n",
    "- Insert size within expected range\n",
    "\n",
    "Improper pairs may indicate:\n",
    "- Structural variants\n",
    "- Chimeric reads\n",
    "- Sequencing artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze proper pairs with 1000bp max insert size\n",
    "max_insert_size = 1000\n",
    "\n",
    "proper_pairs = [f for f in features if f.is_proper_pair(max_insert_size)]\n",
    "paired_reads = [f for f in features if f.is_paired]\n",
    "\n",
    "print(f\"Paired reads: {len(paired_reads)}\")\n",
    "print(f\"Proper pairs: {len(proper_pairs)}\")\n",
    "print(f\"Proper pair rate: {len(proper_pairs) / len(paired_reads) * 100:.1f}%\")\n",
    "\n",
    "# Insert size distribution for proper pairs\n",
    "insert_sizes = [abs(f.template_length) for f in proper_pairs if f.template_length != 0]\n",
    "\n",
    "if insert_sizes:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.hist(insert_sizes, bins=50, color=\"mediumpurple\", edgecolor=\"black\")\n",
    "    plt.xlabel(\"Insert Size (bp)\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(\"Insert Size Distribution (Proper Pairs)\")\n",
    "    plt.axvline(\n",
    "        x=np.mean(insert_sizes),\n",
    "        color=\"r\",\n",
    "        linestyle=\"--\",\n",
    "        label=f\"Mean: {np.mean(insert_sizes):.0f}bp\",\n",
    "    )\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nInsert size statistics:\")\n",
    "    print(f\"  Mean: {np.mean(insert_sizes):.1f}bp\")\n",
    "    print(f\"  Median: {np.median(insert_sizes):.1f}bp\")\n",
    "    print(f\"  Std dev: {np.std(insert_sizes):.1f}bp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Chimeric Read Detection\n",
    "\n",
    "Chimeric reads span multiple loci:\n",
    "- Primary alignment + supplementary alignment(s)\n",
    "- Indicate gene fusions, structural variants, or artifacts\n",
    "\n",
    "SAM flags for chimeric detection:\n",
    "- `0x800`: Supplementary alignment\n",
    "- Primary + supplementary from same read = chimeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count chimeric reads\n",
    "reader3 = dbp.BamReader(\"alignments.bam\", threads=4)\n",
    "chimeric_count = reader3.count_chimeric_reads()\n",
    "\n",
    "print(f\"Chimeric reads detected: {chimeric_count}\")\n",
    "print(f\"Chimeric rate: {chimeric_count / len(features) * 100:.2f}%\")\n",
    "\n",
    "# Typical chimeric rates:\n",
    "# - Normal samples: <0.1%\n",
    "# - Cancer samples with fusions: 0.1-1%\n",
    "# - High chimeric rate may indicate library prep issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. CIGAR String Analysis\n",
    "\n",
    "CIGAR operations describe alignment:\n",
    "- **M**: Match or mismatch\n",
    "- **I**: Insertion (in read)\n",
    "- **D**: Deletion (in read)\n",
    "- **S**: Soft clip (not aligned)\n",
    "- **H**: Hard clip (not present)\n",
    "- **N**: Skipped region (splice junction)\n",
    "\n",
    "High soft clip rates may indicate:\n",
    "- Adapter contamination\n",
    "- Structural variants\n",
    "- Low-quality ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze CIGAR operations\n",
    "total_insertions = sum(f.num_insertions for f in features if f.is_mapped)\n",
    "total_deletions = sum(f.num_deletions for f in features if f.is_mapped)\n",
    "total_soft_clips = sum(f.num_soft_clips for f in features if f.is_mapped)\n",
    "total_hard_clips = sum(f.num_hard_clips for f in features if f.is_mapped)\n",
    "total_aligned = sum(f.aligned_length for f in features if f.is_mapped)\n",
    "\n",
    "print(\"CIGAR Operation Statistics:\")\n",
    "print(f\"  Total aligned bases: {total_aligned}\")\n",
    "print(\n",
    "    f\"  Insertions: {total_insertions} ({total_insertions / total_aligned * 100:.3f}%)\"\n",
    ")\n",
    "print(f\"  Deletions: {total_deletions} ({total_deletions / total_aligned * 100:.3f}%)\")\n",
    "print(\n",
    "    f\"  Soft clips: {total_soft_clips} ({total_soft_clips / total_aligned * 100:.3f}%)\"\n",
    ")\n",
    "print(f\"  Hard clips: {total_hard_clips}\")\n",
    "\n",
    "# Visualize operation frequencies\n",
    "operations = [\"Insertions\", \"Deletions\", \"Soft Clips\"]\n",
    "counts = [total_insertions, total_deletions, total_soft_clips]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(operations, counts, color=[\"indianred\", \"steelblue\", \"orange\"])\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"CIGAR Operation Distribution\")\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Export to Arrow/Parquet\n",
    "\n",
    "Export alignment features to columnar format for:\n",
    "- pandas/polars analysis\n",
    "- DuckDB queries\n",
    "- ML feature engineering\n",
    "\n",
    "Arrow format provides:\n",
    "- Zero-copy sharing\n",
    "- Language interoperability\n",
    "- Efficient columnar access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Export to Arrow table\nimport pyarrow as pa\nimport pyarrow.parquet as pq\n\n# Convert features to dictionary format\ndata = {\n    \"mapping_quality\": [f.mapping_quality for f in features],\n    \"aligned_length\": [f.aligned_length for f in features],\n    \"num_matches\": [f.num_matches for f in features],\n    \"num_insertions\": [f.num_insertions for f in features],\n    \"num_deletions\": [f.num_deletions for f in features],\n    \"num_soft_clips\": [f.num_soft_clips for f in features],\n    \"identity\": [f.identity() for f in features],\n    \"is_mapped\": [f.is_mapped for f in features],\n    \"is_paired\": [f.is_paired for f in features],\n}\n\n# Create Arrow table\ntable = pa.table(data)\n\nprint(f\"Arrow table created: {table.shape[0]} rows, {table.shape[1]} columns\")\nprint(\"\\nSchema:\")\nprint(table.schema)\n\n# Write to Parquet\npq.write_table(table, \"alignment_features.parquet\")\nprint(\"\\nExported to alignment_features.parquet\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Integration with pandas\n",
    "\n",
    "Convert to pandas for exploratory analysis and ML feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Arrow to pandas\n",
    "df = table.to_pandas()\n",
    "\n",
    "print(\"DataFrame preview:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nDescriptive statistics:\")\n",
    "print(df.describe())\n",
    "\n",
    "# Filter high-quality alignments\n",
    "high_quality = df[(df[\"mapping_quality\"] >= 30) & (df[\"identity\"] >= 0.95)]\n",
    "print(f\"\\nHigh-quality alignments: {len(high_quality)} / {len(df)}\")\n",
    "\n",
    "# Feature engineering: indel rate\n",
    "df[\"indel_rate\"] = (df[\"num_insertions\"] + df[\"num_deletions\"]) / df[\"aligned_length\"]\n",
    "df[\"clip_rate\"] = df[\"num_soft_clips\"] / df[\"aligned_length\"]\n",
    "\n",
    "print(\"\\nEngineered features:\")\n",
    "print(df[[\"indel_rate\", \"clip_rate\"]].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Machine Learning Feature Matrix\n",
    "\n",
    "Prepare feature matrix for ML models:\n",
    "- Normalize features\n",
    "- Handle missing values\n",
    "- Create binary quality labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Select numeric features\n",
    "feature_columns = [\n",
    "    \"mapping_quality\",\n",
    "    \"aligned_length\",\n",
    "    \"num_matches\",\n",
    "    \"num_insertions\",\n",
    "    \"num_deletions\",\n",
    "    \"num_soft_clips\",\n",
    "    \"identity\",\n",
    "    \"indel_rate\",\n",
    "    \"clip_rate\",\n",
    "]\n",
    "\n",
    "X = df[feature_columns].values\n",
    "\n",
    "# Create binary label (high quality = MAPQ >= 30 and identity >= 0.95)\n",
    "y = ((df[\"mapping_quality\"] >= 30) & (df[\"identity\"] >= 0.95)).astype(int)\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(\"Label distribution:\")\n",
    "print(f\"  High quality: {y.sum()} ({y.sum() / len(y) * 100:.1f}%)\")\n",
    "print(\n",
    "    f\"  Low quality: {(~y.astype(bool)).sum()} ({(~y.astype(bool)).sum() / len(y) * 100:.1f}%)\"\n",
    ")\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(\"\\nScaled feature matrix ready for ML models\")\n",
    "print(f\"Shape: {X_scaled.shape}\")\n",
    "print(f\"Mean: {X_scaled.mean(axis=0)}\")\n",
    "print(f\"Std: {X_scaled.std(axis=0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "DeepBioP provides comprehensive BAM processing for ML pipelines:\n",
    "\n",
    "| Feature | Purpose | ML Application |\n",
    "|---------|---------|----------------|\n",
    "| Multithreaded reading | Fast I/O | Large-scale data processing |\n",
    "| AlignmentFeatures | Quality metrics | Feature engineering |\n",
    "| Filtering | Quality control | Training data curation |\n",
    "| Chimeric detection | Structural variants | Fusion gene detection |\n",
    "| Arrow export | Interoperability | Integration with pandas/DuckDB |\n",
    "| Batch processing | Efficiency | Production pipelines |\n",
    "\n",
    "Benefits:\n",
    "- ✅ Fast multithreaded BAM decompression\n",
    "- ✅ Rich alignment quality metrics\n",
    "- ✅ Flexible filtering and analysis\n",
    "- ✅ Zero-copy Arrow export\n",
    "- ✅ Ready for scikit-learn/PyTorch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
